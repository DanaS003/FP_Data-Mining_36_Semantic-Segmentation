{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "BN4RZDocNpR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eLzgCBeNaai",
        "outputId": "eec147ea-7a5a-4e2d-f255-f9cea34aeb3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Dec  5 08:27:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 5070 Ti     On  |   00000000:01:00.0 Off |                  N/A |\n",
            "| 32%   38C    P8             29W /  300W |       0MiB /  16303MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z70rx4ksNaaj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubj2aNzoNaak",
        "outputId": "054347af-ca93-4bc1-e0b3-9653db828343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Packages downloaded.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib\n",
        "%pip install gdown\n",
        "clear_output()\n",
        "print(\"Packages downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg8NURryNaak",
        "outputId": "a5d75697-283f-4498-bf55-32d69f97a678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena proses unduh langsung dari Google Drive sering terkena batas too many access, maka digunakan metode unduhan berbasis token autentikasi agar proses download tetap stabil dan berhasil"
      ],
      "metadata": {
        "id": "DxuQdUUzOYPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5GovV93Naak",
        "outputId": "5b943169-aa3c-4fb5-a65e-2fd64fa3b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ Folder dataset sudah ada. Skip download & unzip.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "WORKDIR=$(pwd)\n",
        "DATASET_DIR=\"$WORKDIR/dataset\"\n",
        "DATASET_ZIP=\"$WORKDIR/dataset.zip\"\n",
        "\n",
        "if [ -d \"$DATASET_DIR\" ]; then\n",
        "    echo \"ðŸ“ Folder dataset sudah ada. Skip download & unzip.\"\n",
        "\n",
        "elif [ -f \"$DATASET_ZIP\" ]; then\n",
        "    echo \"ðŸ—‚ï¸ ZIP ditemukan tanpa folder. Sedang di-unzip...\"\n",
        "    unzip -q \"$DATASET_ZIP\" -d \"$DATASET_DIR\"\n",
        "    rm \"$DATASET_ZIP\"\n",
        "    echo \"âœ… Unzip selesai.\"\n",
        "\n",
        "else\n",
        "    echo \"â¬‡ï¸ Download dataset dari Google Drive...\"\n",
        "    curl -L \\\n",
        "      -H \"Authorization: Bearer TOKEN_SAYA\" \\\n",
        "      \"https://www.googleapis.com/drive/v3/files/1kaKoOooBB-r-TVsSIl_HZV6whnx7OFr8?alt=media\" \\\n",
        "      -o \"$DATASET_ZIP\"\n",
        "\n",
        "    echo \"ðŸ› ï¸ Unzip dataset...\"\n",
        "    unzip -q \"$DATASET_ZIP\" -d \"$DATASET_DIR\"\n",
        "    rm \"$DATASET_ZIP\"\n",
        "    echo \"âœ… Selesai download & unzip.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUzsYP0cNaak",
        "outputId": "9e615cb5-155e-4d2b-aea3-8119a2927217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernel Python: /venv/main/bin/python\n",
            "Requirement already satisfied: pip in /venv/main/lib/python3.12/site-packages (25.3)\n",
            "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /venv/main/lib/python3.12/site-packages (0.45.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /venv/main/lib/python3.12/site-packages (3.10.7)\n",
            "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /venv/main/lib/python3.12/site-packages (11.0.0)\n",
            "Requirement already satisfied: opencv-python in /venv/main/lib/python3.12/site-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-image in /venv/main/lib/python3.12/site-packages (0.25.2)\n",
            "Requirement already satisfied: scikit-learn in /venv/main/lib/python3.12/site-packages (1.7.2)\n",
            "Requirement already satisfied: scipy in /venv/main/lib/python3.12/site-packages (1.16.3)\n",
            "Requirement already satisfied: timm in /venv/main/lib/python3.12/site-packages (1.0.22)\n",
            "Requirement already satisfied: torchmetrics in /venv/main/lib/python3.12/site-packages (1.8.2)\n",
            "Requirement already satisfied: transformers in /venv/main/lib/python3.12/site-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /venv/main/lib/python3.12/site-packages (1.12.0)\n",
            "Requirement already satisfied: tokenizers in /venv/main/lib/python3.12/site-packages (0.22.1)\n",
            "Requirement already satisfied: evaluate in /venv/main/lib/python3.12/site-packages (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /venv/main/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: networkx>=3.0 in /venv/main/lib/python3.12/site-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /venv/main/lib/python3.12/site-packages (from scikit-image) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /venv/main/lib/python3.12/site-packages (from scikit-image) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /venv/main/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /venv/main/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /venv/main/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (from timm) (2.8.0+cu128)\n",
            "Requirement already satisfied: torchvision in /venv/main/lib/python3.12/site-packages (from timm) (0.23.0+cu128)\n",
            "Requirement already satisfied: pyyaml in /venv/main/lib/python3.12/site-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /venv/main/lib/python3.12/site-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /venv/main/lib/python3.12/site-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /venv/main/lib/python3.12/site-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface_hub->timm) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /venv/main/lib/python3.12/site-packages (from evaluate) (4.4.1)\n",
            "Requirement already satisfied: dill in /venv/main/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
            "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /venv/main/lib/python3.12/site-packages (from evaluate) (0.70.18)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /venv/main/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /venv/main/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: anyio in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
            "Requirement already satisfied: certifi in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
            "Requirement already satisfied: idna in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /venv/main/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch->timm) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch->timm) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch->timm) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch->timm) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Kernel Python:\", sys.executable)\n",
        "\n",
        "# upgrade pip/tooling\n",
        "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# common data / vision libs\n",
        "!{sys.executable} -m pip install pandas numpy matplotlib tqdm pillow opencv-python scikit-image scikit-learn scipy timm torchmetrics transformers accelerate tokenizers evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oa-RsvENaal"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PybXbS0Naal",
        "outputId": "a928a4a1-99fe-4d7a-fc2f-d44534cc1c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy version: 2.1.2\n",
            "Scipy version: 1.16.3\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import scipy\n",
        "print(f\"Numpy version: {numpy.__version__}\")\n",
        "print(f\"Scipy version: {scipy.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "9pLG1jQ0Naal"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMgC1B6WNaam",
        "outputId": "b4dd0fc8-2d82-43d1-ea9b-e49b6e19d502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ROOT_INP = \"/workspace/dataset\"\n",
        "WORKDIR = \"/workspace\"\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "print(\"DEVICE:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKj4sRnNNaam"
      },
      "source": [
        "## Dataset Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada task semantic segmentation, setiap image selalu dipasangkan dengan mask sebagai training target. Oleh karena itu, path image dan mask diambil serta di-sorted secara terpisah untuk data train, validation, dan test agar tetap saling berkorespondensi."
      ],
      "metadata": {
        "id": "XMhI3BqQO2Xx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpDdJAm2Naam"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "def get_sorted_paths(folder_path):\n",
        "    files = sorted(glob.glob(os.path.join(folder_path, \"*\")))\n",
        "    return files\n",
        "\n",
        "train_image_paths = get_sorted_paths(ROOT_INP+'/train/train-org-img')\n",
        "train_mask_paths = get_sorted_paths(ROOT_INP+'/train/train-label-img')\n",
        "\n",
        "val_image_paths = get_sorted_paths(ROOT_INP+'/val/val-org-img')\n",
        "val_mask_paths = get_sorted_paths(ROOT_INP+'/val/val-label-img')\n",
        "\n",
        "test_image_paths = get_sorted_paths(ROOT_INP+'/test/test-org-img')\n",
        "test_mask_paths = get_sorted_paths(ROOT_INP+'/test/test-label-img')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class RescueNetDataset digunakan untuk memuat imageâ€“mask pair pada proses training semantic segmentation. Image diproses dengan transform, sedangkan mask di-resize dan dikonversi menjadi tensor label sebagai training target."
      ],
      "metadata": {
        "id": "wYiACNqRPLsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIEUBwKiNaam"
      },
      "outputs": [],
      "source": [
        "class RescueNetDataset(Dataset):\n",
        "    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n",
        "        self.image_path = image_path\n",
        "        self.mask_path = mask_path\n",
        "        self.transform = transform\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_path[idx]).convert('RGB')\n",
        "        mask = Image.open(self.mask_path[idx]).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        mask = mask.resize(self.image_size, Image.NEAREST)\n",
        "        mask = np.array(mask, dtype=np.int64)\n",
        "        # mask = np.clip(mask, 0, 9)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCcc2WAmNaam"
      },
      "source": [
        "## Dataset Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini digunakan transform Resize (512Ã—512) dan ToTensor untuk menyeragamkan ukuran input model. Dataset kemudian dimuat ke dalam DataLoader dengan batch size = 2 karena keterbatasan sumber daya untuk proses training, validation, dan testing."
      ],
      "metadata": {
        "id": "PGYV4dOwPhjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9NzaavMNaam"
      },
      "outputs": [],
      "source": [
        "train_test_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = RescueNetDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\n",
        "val_dataset = RescueNetDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\n",
        "test_dataset = RescueNetDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Create Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OChkz5i4Naam"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada bagian ini digunakan Mask2Former dengan backbone Swin-Large pretrained ADE20K sebagai model segmentasi. Image processor disesuaikan tanpa resize dan rescale agar sesuai dengan preprocessing dataset, sementara jumlah output kelas diset menjadi 11 dengan penyesuaian ukuran layer secara otomatis."
      ],
      "metadata": {
        "id": "FR6dsIdfRhGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mEreM_vNaam",
        "outputId": "b105f195-ad64-4ae8-d319-668a8e68f259"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-ade-semantic and are newly initialized because the shapes did not match:\n",
            "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([12, 256]) in the model instantiated\n",
            "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import (Mask2FormerForUniversalSegmentation , Mask2FormerImageProcessor)\n",
        "\n",
        "model_id = \"facebook/mask2former-swin-large-ade-semantic\"\n",
        "\n",
        "processor = Mask2FormerImageProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    ignore_index=255,\n",
        "    do_resize=False,\n",
        "    do_rescale=False)\n",
        "\n",
        "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=11,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXvNjbkMNaam"
      },
      "source": [
        "## Train n Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa_zHozoNaam",
        "outputId": "c2a2f418-d19c-45d7-fa96-2999e219f40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Model facebook/mask2former-swin-large-ade-semantic siap untuk training 11 kelas RescueNet.\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model.to(device)\n",
        "print(f\"Model {model_id} siap untuk training 11 kelas RescueNet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Dv5DXuNaan",
        "outputId": "1a53e409-1c8a-4d16-d265-1d98e099deb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Images: 3595, Train Masks: 3595\n",
            "Val Images: 449, Val Masks: 449\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\n",
        "print(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagian ini mengatur full training loop untuk Mask2Former, termasuk resume dari checkpoint, evaluasi val mIoU (Jaccard Index), dan penyimpanan best model berdasarkan performa validasi.\n",
        "\n",
        "- Jika ditemukan checkpoint lengkap (checkpoint_mask2former.pth), model dan optimizer akan di-load, lalu training dilanjutkan dari start_epoch dengan jumlah epoch tambahan yang ditentukan user. Jika hanya ada best model (best_model_mask2former.pth), sistem menghitung dulu val mIoU saat ini sebelum melanjutkan training.\n",
        "- Selama training, setiap batch imageâ€“mask diproses melalui Mask2FormerImageProcessor, lalu dilewatkan ke model untuk mendapatkan loss dari output; nilai train loss diakumulasikan dan dicatat ke dalam history.\n",
        "- Pada fase validasi, model dijalankan dalam mode eval (tanpa gradient), lalu prediksi di-post-process menjadi peta label, dikonversi dengan to_label_tensor, dan dinilai menggunakan JaccardIndex (mIoU) per-epoch; opsi val loss bisa dihitung jika compute_val_loss = True.\n",
        "\n",
        "- Setelah setiap epoch, dilakukan:\n",
        "\n",
        "  - perhitungan rata-rata train loss dan val loss,\n",
        "  - perhitungan val mIoU,\n",
        "  - penyimpanan checkpoint (model + optimizer + best mIoU), dan\n",
        "  - update best model jika val_miou saat ini lebih tinggi dari sebelumnya (if val_miou > best_val_miou then save model_state_dict).\n",
        "\n",
        "- Di akhir proses, sistem melaporkan best mIoU yang pernah dicapai beserta lokasi file checkpoint dan best model (checkpoint_mask2former.pth dan best_model_mask2former.pth)."
      ],
      "metadata": {
        "id": "Kbt-ZHNkWPPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f3e411a8e5874b8ea0507e3c3c2800db",
            "60507ed560f64af69c0063a0d509c2b2"
          ]
        },
        "id": "QLLdIsQqNaan",
        "outputId": "7e554928-510f-49ed-b0b1-ed29979de457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found checkpoint: checkpoint_mask2former.pth -> loading full checkpoint (model+optimizer)...\n",
            "Resuming from epoch 5 (best mIoU so far = 0.6850)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berapa epoch LAGI mau dijalankan (current start_epoch=5)? [ketik angka, mis. 4] :  1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mulai training dari epoch 5 hingga 5...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3e411a8e5874b8ea0507e3c3c2800db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "[Train] Epoch 6/6:   0%|          | 0/1797 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Train Loss: 8.5985\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60507ed560f64af69c0063a0d509c2b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "[Val] Epoch 6/6:   0%|          | 0/224 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Œ Val Loss: 0.0000 | Val mIoU: 0.6813\n",
            "--------------------------------------------------\n",
            "\n",
            "Training done. Best mIoU = 0.6850. Checkpoint saved to checkpoint_mask2former.pth and best model to best_model_mask2former.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics import JaccardIndex\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 25\n",
        "lr = 1e-5\n",
        "checkpoint_path = \"checkpoint_mask2former.pth\"\n",
        "best_model_path = \"best_model_mask2former.pth\"\n",
        "\n",
        "compute_val_loss = False\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "val_iou_metric = JaccardIndex(task=\"multiclass\", num_classes=11, ignore_index=255).to(device)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "start_epoch = 0\n",
        "best_val_miou = 0.0\n",
        "\n",
        "def save_checkpoint(path, epoch, model, optimizer, best_val_miou):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"best_val_miou\": best_val_miou\n",
        "    }, path)\n",
        "\n",
        "def to_label_tensor(pred_item):\n",
        "\n",
        "    if isinstance(pred_item, np.ndarray):\n",
        "        t = torch.from_numpy(pred_item)\n",
        "    elif isinstance(pred_item, torch.Tensor):\n",
        "        t = pred_item.detach().cpu()\n",
        "    else:\n",
        "        # fallback\n",
        "        t = torch.tensor(pred_item).cpu()\n",
        "\n",
        "    t = t.long()\n",
        "\n",
        "    if t.dim() == 2:\n",
        "        t = t.unsqueeze(0)\n",
        "    elif t.dim() == 3:\n",
        "        if t.shape[0] > 1:\n",
        "            t = t.argmax(dim=0, keepdim=True)\n",
        "        elif t.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            t = t.squeeze(0).unsqueeze(0)\n",
        "    else:\n",
        "        t = t.squeeze()\n",
        "        if t.dim() == 2:\n",
        "            t = t.unsqueeze(0)\n",
        "    return t.long()\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Found checkpoint: {checkpoint_path} -> loading full checkpoint (model+optimizer)...\")\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "    start_epoch = ckpt[\"epoch\"]\n",
        "    best_val_miou = ckpt.get(\"best_val_miou\", 0.0)\n",
        "    print(f\"Resuming from epoch {start_epoch} (best mIoU so far = {best_val_miou:.4f})\")\n",
        "    try:\n",
        "        extra = int(input(f\"Berapa epoch LAGI mau dijalankan (current start_epoch={start_epoch})? [ketik angka, mis. 4] : \"))\n",
        "    except Exception:\n",
        "        extra = 4\n",
        "    target_epoch = start_epoch + extra\n",
        "\n",
        "elif os.path.exists(best_model_path):\n",
        "    print(f\"Found model-only file: {best_model_path} -> loading model weights (optimizer state unknown).\")\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)  # fresh optimizer\n",
        "    start_epoch = 0\n",
        "\n",
        "    print(\"Menghitung Val mIoU sekarang (1 full pass) untuk mengetahui nilai terakhir...\")\n",
        "    model.eval()\n",
        "    val_iou_metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Quick Eval for current mIoU\"):\n",
        "            list_images = [img for img in images]\n",
        "            list_masks  = [m for m in masks]\n",
        "            inputs = processor(images=list_images, segmentation_maps=list_masks,\n",
        "                               task_inputs=[\"semantic\"] * len(images), return_tensors=\"pt\")\n",
        "            pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "            with autocast():\n",
        "                # do inference-only to save mem\n",
        "                try:\n",
        "                    outputs = model(pixel_values=pixel_values)\n",
        "                except TypeError:\n",
        "                    outputs = model(pixel_values=pixel_values,\n",
        "                                    mask_labels=[m.to(device) for m in inputs[\"mask_labels\"]],\n",
        "                                    class_labels=[c.to(device) for c in inputs[\"class_labels\"]])\n",
        "\n",
        "            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n",
        "            preds = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
        "\n",
        "            for pred_item, target in zip(preds, masks):\n",
        "                pred_tensor = to_label_tensor(pred_item).to(device)\n",
        "                if isinstance(target, torch.Tensor):\n",
        "                    target_tensor = target.unsqueeze(0).to(device) if target.dim() == 2 else target.to(device)\n",
        "                else:\n",
        "                    target_tensor = torch.tensor(target).long().unsqueeze(0).to(device)\n",
        "                val_iou_metric.update(pred_tensor, target_tensor)\n",
        "\n",
        "            del inputs, pixel_values, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    current_miou = val_iou_metric.compute().mean().item()\n",
        "    val_iou_metric.reset()\n",
        "    best_val_miou = current_miou\n",
        "    print(f\"Val mIoU saat ini (dari model .pth) = {current_miou:.4f}\")\n",
        "    try:\n",
        "        extra = int(input(f\"Model di-load dari .pth. Mau lanjut berapa epoch lagi? [ketik angka, mis. 4] : \"))\n",
        "    except Exception:\n",
        "        extra = 4\n",
        "    target_epoch = start_epoch + extra\n",
        "\n",
        "else:\n",
        "    print(\"Tidak ada checkpoint ditemukan. Training akan dimulai dari 0.\")\n",
        "    start_epoch = 0\n",
        "    best_val_miou = 0.0\n",
        "    try:\n",
        "        target_epoch = int(input(f\"Tentukan total epoch target (default {EPOCHS}): \") or EPOCHS)\n",
        "    except Exception:\n",
        "        target_epoch = EPOCHS\n",
        "\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
        "global_iter = 0\n",
        "\n",
        "print(f\"Mulai training dari epoch {start_epoch} hingga {target_epoch-1}...\")\n",
        "\n",
        "for epoch in range(start_epoch, target_epoch):\n",
        "    model.train()\n",
        "    epoch_train_loss = 0.0\n",
        "    train_bar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{target_epoch}\")\n",
        "\n",
        "    for images, masks in train_bar:\n",
        "        list_images = [img for img in images]\n",
        "        list_masks  = [m for m in masks]\n",
        "\n",
        "        inputs = processor(images=list_images, segmentation_maps=list_masks,\n",
        "                           task_inputs=[\"semantic\"] * len(images), return_tensors=\"pt\")\n",
        "\n",
        "        pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "        mask_labels  = [m.to(device) for m in inputs[\"mask_labels\"]]\n",
        "        class_labels = [c.to(device) for c in inputs[\"class_labels\"]]\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, mask_labels=mask_labels, class_labels=class_labels)\n",
        "\n",
        "        loss = getattr(outputs, \"loss\", None)\n",
        "        if loss is None:\n",
        "            # unexpected but handle gracefully\n",
        "            batch_loss = 0.0\n",
        "        else:\n",
        "            batch_loss = float(loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if loss is not None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            pass\n",
        "        global_iter += 1\n",
        "\n",
        "        epoch_train_loss += batch_loss\n",
        "        train_bar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
        "\n",
        "        del inputs, pixel_values, mask_labels, class_labels, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
        "    print(f\"ðŸŽ¯ Train Loss: {avg_train_loss:.4f}\")\n",
        "    history[\"train_loss\"].append(avg_train_loss)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_iou_metric.reset()\n",
        "    epoch_val_loss = 0.0\n",
        "\n",
        "    val_bar = tqdm(val_loader, desc=f\"[Val] Epoch {epoch+1}/{target_epoch}\")\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_bar:\n",
        "            list_images = [img for img in images]\n",
        "            list_masks  = [m for m in masks]\n",
        "\n",
        "            inputs = processor(images=list_images, segmentation_maps=list_masks,\n",
        "                               task_inputs=[\"semantic\"] * len(images), return_tensors=\"pt\")\n",
        "\n",
        "            pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "            if compute_val_loss:\n",
        "                mask_labels_dev  = [m.to(device) for m in inputs[\"mask_labels\"]]\n",
        "                class_labels_dev = [c.to(device) for c in inputs[\"class_labels\"]]\n",
        "                with autocast():\n",
        "                    outputs = model(pixel_values=pixel_values, mask_labels=mask_labels_dev, class_labels=class_labels_dev)\n",
        "            else:\n",
        "                with autocast():\n",
        "                    try:\n",
        "                        outputs = model(pixel_values=pixel_values)\n",
        "                    except TypeError:\n",
        "                        # fallback: some models require labels\n",
        "                        outputs = model(pixel_values=pixel_values,\n",
        "                                        mask_labels=[m.to(device) for m in inputs[\"mask_labels\"]],\n",
        "                                        class_labels=[c.to(device) for c in inputs[\"class_labels\"]])\n",
        "\n",
        "            # accumulate val loss only if present and not None\n",
        "            loss_val = getattr(outputs, \"loss\", None)\n",
        "            if loss_val is not None:\n",
        "                epoch_val_loss += float(loss_val)\n",
        "\n",
        "            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n",
        "            preds = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
        "\n",
        "            for pred_item, target in zip(preds, masks):\n",
        "                pred_tensor = to_label_tensor(pred_item).to(device)\n",
        "                if isinstance(target, torch.Tensor):\n",
        "                    target_tensor = target.unsqueeze(0).to(device) if target.dim() == 2 else target.to(device)\n",
        "                else:\n",
        "                    target_tensor = torch.tensor(target).long().unsqueeze(0).to(device)\n",
        "                val_iou_metric.update(pred_tensor, target_tensor)\n",
        "\n",
        "            del inputs, pixel_values, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader) if len(val_loader) > 0 else 0.0\n",
        "    val_miou = val_iou_metric.compute().mean().item()\n",
        "    val_iou_metric.reset()\n",
        "\n",
        "    print(f\"ðŸ“Œ Val Loss: {avg_val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    history[\"val_loss\"].append(avg_val_loss)\n",
        "    history[\"val_miou\"].append(val_miou)\n",
        "\n",
        "    save_checkpoint(checkpoint_path, epoch+1, model, optimizer, max(best_val_miou, val_miou))\n",
        "\n",
        "    if val_miou > best_val_miou:\n",
        "        best_val_miou = val_miou\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"New best model ! Epoch {epoch+1}, mIoU={val_miou:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining done. Best mIoU = {best_val_miou:.4f}. Checkpoint saved to {checkpoint_path} and best model to {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot  loss"
      ],
      "metadata": {
        "id": "UIb_k2aSYGvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SI7z4X4Naan",
        "outputId": "4a032842-2750-46f7-f46a-c3b84fb8d107"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Train Loss vs Val Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.title(\"Train Loss vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh4K0ax4Naan",
        "outputId": "99adde46-b99a-4896-a248-a25d2398e81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved to: /workspace/history_mask2former.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "output_path = WORKDIR + \"/history_mask2former.json\"\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(history, f, indent=4)\n",
        "\n",
        "print(\"File saved to:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CgAD5nANaan"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics import JaccardIndex\n",
        "\n",
        "def test_model(model, test_loader, device, processor):\n",
        "    metric = JaccardIndex(\n",
        "        task=\"multiclass\",\n",
        "        num_classes=11,\n",
        "        ignore_index=255,\n",
        "        average=\"none\"\n",
        "    ).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "            list_images = [img for img in images]\n",
        "\n",
        "            inputs = processor(\n",
        "                images=list_images,\n",
        "                return_tensors=\"pt\",\n",
        "                do_resize=False,\n",
        "                do_rescale=False\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n",
        "            pred_maps = processor.post_process_semantic_segmentation(\n",
        "                outputs, target_sizes=target_sizes\n",
        "            )\n",
        "            preds_batch = torch.stack(pred_maps).to(device)\n",
        "            target_batch = masks.to(device)\n",
        "\n",
        "            metric.update(preds_batch, target_batch)\n",
        "\n",
        "    iou_per_class = metric.compute()\n",
        "\n",
        "    mIoU = iou_per_class.mean().item()\n",
        "\n",
        "    print(\"\\n=== HASIL TESTING ===\")\n",
        "    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    class_names = [\n",
        "        \"Background\", \"Water\", \"Building No Damage\", \"Building Minor Damage\",\n",
        "        \"Building Major Damage\", \"Building Total Destruction\", \"Road-Clear\",\n",
        "        \"Road-Blocked\", \"Vehicle\", \"Tree\", \"Pool\"\n",
        "    ]\n",
        "\n",
        "    for i, iou in enumerate(iou_per_class):\n",
        "        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n",
        "        print(f\"{name:25s}: {iou.item():.4f}\")\n",
        "\n",
        "    metric.reset()\n",
        "    return mIoU, iou_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9aaf9d4a7fda453e81c2a940e321b5b0"
          ]
        },
        "id": "SN5sTKFONaan",
        "outputId": "c08f7cf4-e389-4389-bbaf-d7d0f469f7f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-ade-semantic and are newly initialized because the shapes did not match:\n",
            "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([12, 256]) in the model instantiated\n",
            "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model keys: 782; Compatible keys from ckpt: 782; Skipped keys: 0\n",
            "Mulai Testing (menggunakan JaccardIndex)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9aaf9d4a7fda453e81c2a940e321b5b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing:   0%|          | 0/225 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== HASIL TESTING ===\n",
            "Mean IoU (mIoU): 0.6943\n",
            "------------------------------\n",
            "Background               : 0.8549\n",
            "Water                    : 0.8589\n",
            "Building No Damage       : 0.6991\n",
            "Building Minor Damage    : 0.5790\n",
            "Building Major Damage    : 0.5612\n",
            "Building Total Destruction: 0.6328\n",
            "Road-Clear               : 0.6394\n",
            "Road-Blocked             : 0.7677\n",
            "Vehicle                  : 0.4277\n",
            "Tree                     : 0.8355\n",
            "Pool                     : 0.7815\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n",
        "\n",
        "model_id = \"facebook/mask2former-swin-large-ade-semantic\"\n",
        "\n",
        "class_names = [\n",
        "    \"Background\", \"Water\", \"Building No Damage\", \"Building Minor Damage\",\n",
        "    \"Building Major Damage\", \"Building Total Destruction\", \"Road-Clear\",\n",
        "    \"Road-Blocked\", \"Vehicle\", \"Tree\", \"Pool\"\n",
        "]\n",
        "num_labels = len(class_names)\n",
        "num_classes_internal = num_labels + 1\n",
        "\n",
        "processor = Mask2FormerImageProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    ignore_index=255,\n",
        "    do_resize=False,\n",
        "    do_rescale=False\n",
        ")\n",
        "\n",
        "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "ckpt = torch.load(WORKDIR + \"/best_model_mask2former.pth\", map_location=\"cpu\")\n",
        "\n",
        "if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
        "    ckpt_state = ckpt[\"model\"]\n",
        "else:\n",
        "    ckpt_state = ckpt\n",
        "\n",
        "model_state = model.state_dict()\n",
        "compatible = {k: v for k, v in ckpt_state.items() if (k in model_state and v.shape == model_state[k].shape)}\n",
        "\n",
        "print(f\"Total model keys: {len(model_state)}; Compatible keys from ckpt: {len(compatible)}; Skipped keys: {len(ckpt_state)-len(compatible)}\")\n",
        "\n",
        "model_state.update(compatible)\n",
        "model.load_state_dict(model_state)\n",
        "\n",
        "test_mIoU, test_iou_per_class = test_model(model, test_loader, device, processor)\n",
        "\n",
        "# print(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\n",
        "# print(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\n",
        "# print(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\n",
        "# print(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\n",
        "# print(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\\n\\n\")\n",
        "# print(f\"===================== IoU Per Class ========================\")\n",
        "# for i, iou in enumerate(test_iou_per_class):\n",
        "#         name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n",
        "#         print(f\"{name:25s}: {iou.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn72G_CRNaan"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSIFqxmvNaan"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import os\n",
        "# from matplotlib.patches import Patch\n",
        "\n",
        "# CLASS_NAMES = [\n",
        "#     \"Background\",\n",
        "#     \"Water\",\n",
        "#     \"Building No Damage\",\n",
        "#     \"Building Minor Damage\",\n",
        "#     \"Building Major Damage\",\n",
        "#     \"Building Total Destruction\",\n",
        "#     \"Road-Clear\",\n",
        "#     \"Road-Blocked\",\n",
        "#     \"Vehicle\",\n",
        "#     \"Tree\",\n",
        "#     \"Pool\"\n",
        "# ]\n",
        "\n",
        "# LABEL_COLORS = np.array([\n",
        "#     [0, 0, 0],         # Background\n",
        "#     [30, 230, 255],    # Water\n",
        "#     [184, 115, 117],   # Building No Damage\n",
        "#     [216, 255, 0],     # Building Minor Damage\n",
        "#     [252, 199, 0],     # Building Major Damage\n",
        "#     [255, 0, 0],       # Building Total Destruction\n",
        "#     [140, 140, 140],   # Road-Clear\n",
        "#     [151, 0, 255],     # Road-Blocked\n",
        "#     [255, 0, 246],     # Vehicle\n",
        "#     [0, 255, 0],       # Tree\n",
        "#     [244, 255, 0]      # Pool\n",
        "# ])\n",
        "# def decode_segmap(mask):\n",
        "#     r = np.zeros_like(mask).astype(np.uint8)\n",
        "#     g = np.zeros_like(mask).astype(np.uint8)\n",
        "#     b = np.zeros_like(mask).astype(np.uint8)\n",
        "\n",
        "#     for l in range(0, len(LABEL_COLORS)):\n",
        "#         idx = mask == l\n",
        "#         r[idx] = LABEL_COLORS[l, 0]\n",
        "#         g[idx] = LABEL_COLORS[l, 1]\n",
        "#         b[idx] = LABEL_COLORS[l, 2]\n",
        "\n",
        "#     rgb = np.stack([r, g, b], axis=2)\n",
        "#     return rgb\n",
        "\n",
        "# def find_indices_by_filename(dataset, target_ids):\n",
        "#     found_indices = []\n",
        "#     for target in target_ids:\n",
        "#         found = False\n",
        "#         for idx, path in enumerate(dataset.image_path):\n",
        "#             if str(target) in os.path.basename(path):\n",
        "#                 found_indices.append(idx)\n",
        "#                 found = True\n",
        "#                 break\n",
        "#         if not found:\n",
        "#             return\n",
        "#     return found_indices\n",
        "\n",
        "# def visualize_specific_images(model, dataset, target_ids, device, processor):\n",
        "#     model.eval()\n",
        "\n",
        "#     indices = find_indices_by_filename(dataset, target_ids)\n",
        "\n",
        "#     num_samples = len(indices)\n",
        "#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n",
        "\n",
        "#     if num_samples == 1:\n",
        "#         axes = axes.reshape(1, -1)\n",
        "\n",
        "#     for row_idx, idx in enumerate(indices):\n",
        "#         image, mask = dataset[idx]\n",
        "\n",
        "#         filename = os.path.basename(dataset.image_path[idx])\n",
        "\n",
        "#         inputs = processor(\n",
        "#             images=[image],\n",
        "#             return_tensors=\"pt\",\n",
        "#             do_resize=False,\n",
        "#             do_rescale=False\n",
        "#         )\n",
        "#         inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**inputs)\n",
        "\n",
        "#         target_sizes = [(mask.shape[0], mask.shape[1])]\n",
        "#         pred_map = processor.post_process_semantic_segmentation(\n",
        "#             outputs, target_sizes=target_sizes\n",
        "#         )[0]\n",
        "\n",
        "#         img_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "#         mask_rgb = decode_segmap(mask.numpy())\n",
        "#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n",
        "\n",
        "#         axes[row_idx, 0].imshow(img_np)\n",
        "#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n",
        "#         axes[row_idx, 0].axis(\"off\")\n",
        "\n",
        "#         axes[row_idx, 1].imshow(mask_rgb)\n",
        "#         axes[row_idx, 1].set_title(\"Ground Truth\")\n",
        "#         axes[row_idx, 1].axis(\"off\")\n",
        "\n",
        "#         axes[row_idx, 2].imshow(pred_rgb)\n",
        "#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n",
        "#         axes[row_idx, 2].axis(\"off\")\n",
        "\n",
        "#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n",
        "#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
        "\n",
        "#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.subplots_adjust(bottom=0.08)\n",
        "#     plt.show()\n",
        "\n",
        "# target_ids = [\"10794\", \"10801\", \"10807\"]\n",
        "\n",
        "# visualize_specific_images(model, test_dataset, target_ids, device, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnYgu_PFNaao"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# test_imgs, test_masks = next(iter(test_loader))\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n",
        "\n",
        "#     outputs = model(inputs)\n",
        "\n",
        "#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n",
        "# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8902802,
          "sourceId": 13965826,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
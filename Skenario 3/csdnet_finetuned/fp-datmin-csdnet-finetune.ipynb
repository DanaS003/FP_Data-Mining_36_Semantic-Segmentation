{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:34.810820Z","iopub.execute_input":"2025-12-03T14:22:34.811519Z","iopub.status.idle":"2025-12-03T14:22:34.816285Z","shell.execute_reply.started":"2025-12-03T14:22:34.811488Z","shell.execute_reply":"2025-12-03T14:22:34.815620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:34.817590Z","iopub.execute_input":"2025-12-03T14:22:34.817771Z","iopub.status.idle":"2025-12-03T14:22:34.830893Z","shell.execute_reply.started":"2025-12-03T14:22:34.817758Z","shell.execute_reply":"2025-12-03T14:22:34.830158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{}},{"cell_type":"code","source":"ls /kaggle/input\n\nNUM_CLASSES = 10\nBATCH_SIZE = 16\nEPOCHS = 25\n\nMODEL_NAME = \"best_model_csdnet.pth\"\nMODEL_NAME_FINETUNED = \"best_model_csdnet_finetuned.pth\"\nDIR_MODEL = \"/kaggle/input/csdnet/pytorch/default/1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:34.831622Z","iopub.execute_input":"2025-12-03T14:22:34.831834Z","iopub.status.idle":"2025-12-03T14:22:35.030691Z","shell.execute_reply.started":"2025-12-03T14:22:34.831811Z","shell.execute_reply":"2025-12-03T14:22:35.029790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport re\ndef sort_files_numerically(directory):\n    files = os.listdir(directory)\n    files_sorted = sorted(files, key=lambda x: int(re.search(r'\\d+', x).group()))\n    return [os.path.join(directory, f) for f in files_sorted]\n\nROOT_INP = \"/kaggle/input/indo-flood-segmentation-dataset\"\n\ntrain_image_paths = sort_files_numerically(ROOT_INP+'/train/train-org-img')\ntrain_mask_paths = sort_files_numerically(ROOT_INP+'/train/train-label-img')\n\nval_image_paths = sort_files_numerically(ROOT_INP+'/val/val-org-img')\nval_mask_paths = sort_files_numerically(ROOT_INP+'/val/val-label-img')\n\ntest_image_paths = sort_files_numerically(ROOT_INP+'/test/test-org-img')\ntest_mask_paths = sort_files_numerically(ROOT_INP+'/test/test-label-img')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:35.031881Z","iopub.execute_input":"2025-12-03T14:22:35.032204Z","iopub.status.idle":"2025-12-03T14:22:35.063444Z","shell.execute_reply.started":"2025-12-03T14:22:35.032172Z","shell.execute_reply":"2025-12-03T14:22:35.062695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FloodDataset(Dataset):\n    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.transform = transform\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_path[idx]).convert('RGB')\n        mask = Image.open(self.mask_path[idx]).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        mask = mask.resize(self.image_size, Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        mask = np.clip(mask, 0, 9)\n        mask = torch.from_numpy(mask).long()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:35.065549Z","iopub.execute_input":"2025-12-03T14:22:35.065928Z","iopub.status.idle":"2025-12-03T14:22:35.072224Z","shell.execute_reply.started":"2025-12-03T14:22:35.065912Z","shell.execute_reply":"2025-12-03T14:22:35.071499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"train_test_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = FloodDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\nval_dataset = FloodDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\ntest_dataset = FloodDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:35.072799Z","iopub.execute_input":"2025-12-03T14:22:35.073468Z","iopub.status.idle":"2025-12-03T14:22:35.090099Z","shell.execute_reply.started":"2025-12-03T14:22:35.073452Z","shell.execute_reply":"2025-12-03T14:22:35.089381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_all_unique_masks(dataloader, max_batches=None):\n#     all_unique = set()\n#     for i, (_, masks) in enumerate(dataloader):\n#         all_unique.update(int(u) for u in torch.unique(masks))\n#         if max_batches is not None and i+1 >= max_batches:\n#             break\n#     return sorted(all_unique)\n\n# # print(\"Train unique (scan 200 batches):\", get_all_unique_masks(train_loader, max_batches=200))\n# # print(\"Val unique   (scan all):       \", get_all_unique_masks(val_loader))\n# print(\"Test unique  (scan all):       \", get_all_unique_masks(test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:22:35.113521Z","iopub.execute_input":"2025-12-03T14:22:35.113718Z","iopub.status.idle":"2025-12-03T14:23:35.679965Z","shell.execute_reply.started":"2025-12-03T14:22:35.113705Z","shell.execute_reply":"2025-12-03T14:23:35.679168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"import timm\n\nclass DWSC(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, \n                                   padding, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        x = self.bn(x)\n        return self.relu(x)\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels=256):\n        super().__init__()\n        dilations = [1, 6, 12, 18]\n        self.aspp_blocks = nn.ModuleList()\n        \n        self.aspp_blocks.append(nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ))\n\n        for dilation in dilations[1:]:\n            self.aspp_blocks.append(nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ))\n\n        self.global_pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        self.project = nn.Sequential(\n            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5)\n        )\n\n    def forward(self, x):\n        res = []\n        for block in self.aspp_blocks:\n            res.append(block(x))\n        \n        g = self.global_pool(x)\n        g = F.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=False)\n        res.append(g)\n        \n        res = torch.cat(res, dim=1)\n        return self.project(res)\n\nclass TargetedEnhancementModule(nn.Module):\n    def __init__(self, f1_channels, detector_channels=256, fusion_dim=256):\n        super().__init__()\n        self.aspp_f1 = ASPP(f1_channels, fusion_dim)\n        \n        self.phi = nn.Sequential(\n            nn.Conv2d(detector_channels, fusion_dim, 1),\n            nn.BatchNorm2d(fusion_dim),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, f1, f_det):\n        \n        f1_processed = self.aspp_f1(f1)\n        \n        f_det_processed = self.phi(f_det)\n        f_det_resized = F.interpolate(f_det_processed, size=f1_processed.shape[2:], \n                                      mode='bilinear', align_corners=False)\n        \n        return f1_processed * f_det_resized\n\nclass DeepContextualAttention(nn.Module):\n    def __init__(self, in_channels, dim=256):\n        super().__init__()\n        self.dwsc_in = DWSC(in_channels, dim)\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=4, dim_feedforward=dim*2, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        self.aspp = ASPP(dim, dim)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        \n        x = self.dwsc_in(x) # (B, dim, H, W)\n        \n        tokens = x.flatten(2).transpose(1, 2)\n        \n        tokens = self.transformer(tokens)\n        \n        x_trans = tokens.transpose(1, 2).view(B, -1, H, W)\n        \n        x_out = self.aspp(x_trans)\n        \n        return x_out\n\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, f2_channels, f3_channels, out_dim=256):\n        super().__init__()\n        self.psi2 = nn.Conv2d(f2_channels, out_dim, 1)\n        self.psi3 = nn.Conv2d(f3_channels, out_dim, 1)\n        \n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(out_dim * 2, out_dim, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, f2, f3):\n        f2_hat = self.psi2(f2)\n        f3_hat = self.psi3(f3)\n        \n        f3_hat_up = F.interpolate(f3_hat, size=f2_hat.shape[2:], mode='bilinear', align_corners=False)\n        \n        f_gen = torch.cat([f2_hat, f3_hat_up], dim=1)\n        return self.fusion_conv(f_gen)\n\nclass CSDNet(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        \n        self.encoder = timm.create_model(\n            \"efficientnet_b5\",\n            pretrained=True,\n            features_only=True,\n            out_indices=(0, 1, 2, 3) \n        )\n        \n        dims = self.encoder.feature_info.channels() \n        f1_c, f2_c, f3_c, f4_c = dims[0], dims[1], dims[2], dims[3]\n        \n        fusion_dim = 128 \n        \n        self.mod1_detection = TargetedEnhancementModule(f1_c, detector_channels=256, fusion_dim=fusion_dim)\n        \n        self.mod2_transformer = DeepContextualAttention(f4_c, dim=fusion_dim)\n        \n        self.mod3_cnn = MultiScaleFusion(f2_c, f3_c, out_dim=fusion_dim)\n        \n        self.classifier = nn.Sequential(\n            nn.Conv2d(fusion_dim * 3, 256, 3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(256, num_classes, 1)\n        )\n        \n        self.detector_dummy_layer = nn.Conv2d(3, 256, kernel_size=32, stride=32) \n\n    def get_detection_features(self, x):\n        with torch.no_grad(): \n            det_feats = self.detector_dummy_layer(x) \n        return det_feats\n\n    def forward(self, x):\n        input_shape = x.shape[2:]\n        \n        feats = self.encoder(x)\n        f1, f2, f3, f4 = feats[0], feats[1], feats[2], feats[3]\n        \n        f_det_raw = self.get_detection_features(x)\n        \n        feat_branch1 = self.mod1_detection(f1, f_det_raw) \n        \n        feat_branch2 = self.mod2_transformer(f4)\n        \n        feat_branch3 = self.mod3_cnn(f2, f3)\n        \n        target_size = feat_branch1.shape[2:]\n        \n        feat_branch2_up = F.interpolate(feat_branch2, size=target_size, mode='bilinear', align_corners=False)\n        feat_branch3_up = F.interpolate(feat_branch3, size=target_size, mode='bilinear', align_corners=False)\n        \n        f_final = torch.cat([feat_branch1, feat_branch2_up, feat_branch3_up], dim=1)\n        \n        logits = self.classifier(f_final)\n        \n        logits = F.interpolate(logits, size=input_shape, mode='bilinear', align_corners=False)\n        \n        return logits\n\n# model = CSDNet(num_classes=NUM_CLASSES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:35.681040Z","iopub.execute_input":"2025-12-03T14:23:35.681293Z","iopub.status.idle":"2025-12-03T14:23:35.694278Z","shell.execute_reply.started":"2025-12-03T14:23:35.681269Z","shell.execute_reply":"2025-12-03T14:23:35.693530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train n Eval","metadata":{}},{"cell_type":"code","source":"print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\nprint(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"DEVICE USED : {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:35.695018Z","iopub.execute_input":"2025-12-03T14:23:35.695270Z","iopub.status.idle":"2025-12-03T14:23:35.716688Z","shell.execute_reply.started":"2025-12-03T14:23:35.695247Z","shell.execute_reply":"2025-12-03T14:23:35.716001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CSDNet(num_classes=NUM_CLASSES)\n\nif os.path.exists(WEIGHTS_PATH):\n    print(f\"Loading weights from: {WEIGHTS_PATH}\")\n    checkpoint = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n    \n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n\n    new_state_dict = {}\n    \n    for key, value in state_dict.items():\n        if \"classifier.4\" in key:\n            print(f\"Skipping weight: {key} (Shape mismatch expected: Old vs New classes)\")\n            continue\n            \n        new_state_dict[key] = value\n\n    msg = model.load_state_dict(new_state_dict, strict=False)\n    \n    print(\"\\nStatus Load Model:\")\n    print(msg) \n\n    \nelse:\n    print(f\"Warning: File {WEIGHTS_PATH} tidak ditemukan.\")\n    \nmodel.to(device)\n\nlast_layer = model.classifier[4]\nprint(f\"\\nVerifikasi Output Layer: {last_layer.out_channels} channels (Harus sama dengan {NEW_NUM_CLASSES})\")\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None, ignore_index=255):\n        super().__init__()\n        self.gamma = gamma\n        self.ce = nn.CrossEntropyLoss(reduction='none', weight=alpha, ignore_index=ignore_index)\n\n    def forward(self, logits, targets):\n        ce_loss = self.ce(logits, targets) \n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean() \n\n\nclass JaccardLoss(nn.Module):\n    def __init__(self, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, logits, targets):\n        num_classes = logits.shape[1]\n        preds = torch.softmax(logits, dim=1)\n\n        target_1hot = torch.nn.functional.one_hot(targets, num_classes).permute(0,3,1,2)\n\n        intersection = (preds * target_1hot).sum(dim=(2,3))\n        union = preds.sum(dim=(2,3)) + target_1hot.sum(dim=(2,3)) - intersection\n\n        iou = (intersection + self.eps) / (union + self.eps)\n        return 1 - iou.mean()\n\n\nfocal_loss = FocalLoss(gamma=2.0)\njaccard_loss = JaccardLoss()\n\n\ndef total_loss_fn(pred, target):\n    return focal_loss(pred, target) + jaccard_loss(pred, target)\n\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=1e-4\n)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    patience=10,\n    factor=0.1,\n    min_lr=1e-4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:35.717400Z","iopub.execute_input":"2025-12-03T14:23:35.717600Z","iopub.status.idle":"2025-12-03T14:23:46.540712Z","shell.execute_reply.started":"2025-12-03T14:23:35.717586Z","shell.execute_reply":"2025-12-03T14:23:46.539926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics import JaccardIndex\nfrom tqdm.auto import tqdm\n\nmetric_val   = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(device)\n\nbest_mIoU = 0.0\n\nhistory = {\n    \"train_loss\": [].\n    \"val_loss\": [],\n    \"val_miou\": []\n}\n\nfor epoch in range(EPOCHS):\n\n    model.train()\n    train_loss = 0\n    \n    if hasattr(model, 'detector_dummy_layer'):\n        model.detector_dummy_layer.eval()\n\n    for images, masks in tqdm(train_loader, desc=f\"Train {epoch+1}/{EPOCHS}\"):\n        images, masks = images.to(device), masks.to(device)\n\n        optimizer.zero_grad()\n        preds = model(images)\n        loss = total_loss_fn(preds, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n\n    print(f\"ðŸŽ¯ Train Loss: {train_loss:.4f}\")\n    history[\"train_loss\"].append(train_loss)\n\n    model.eval()\n    val_loss = 0\n    metric_val.reset()\n\n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=f\"Val {epoch+1}/{EPOCHS}\"):\n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n            loss = total_loss_fn(preds, masks)\n            val_loss += loss.item()\n\n            pred_mask = preds.argmax(dim=1)\n            metric_val.update(pred_mask, masks)\n\n    val_loss /= len(val_loader)\n    iou_val_per_class = metric_val.compute()\n    mIoU_val = iou_val_per_class.mean().item()\n\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_miou\"].append(mIoU_val)\n\n    print(f\"\"\"\n============================== Epoch: {epoch+1}/{EPOCHS}\nTrain Loss: {train_loss:.4f}\nVal Loss:   {val_loss:.4f}   | Val mIoU: {mIoU_val:.4f}\n==============================\n\"\"\")\n\n\n    scheduler.step(val_loss)\n\n    if best_mIoU < mIoU_val:\n        best_mIoU = mIoU_val\n        torch.save(model.state_dict(), \"best_model_csdnet_finetuned.pth\")\n        print(\"Model disimpan (best so far).\")\n            \nprint(\"Training Selesai!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:23:46.542312Z","iopub.execute_input":"2025-12-03T14:23:46.542504Z","iopub.status.idle":"2025-12-03T14:41:27.650524Z","shell.execute_reply.started":"2025-12-03T14:23:46.542490Z","shell.execute_reply":"2025-12-03T14:41:27.649594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nWORKDIR=\"/kaggle/working\"\noutput_path = WORKDIR+\"/history_csdnet_finetuned.json\"\n\nwith open(output_path, \"w\") as f:\n    json.dump(history, f, indent=4)\n\nprint(\"File saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:41:27.651990Z","iopub.execute_input":"2025-12-03T14:41:27.652235Z","iopub.status.idle":"2025-12-03T14:41:27.658371Z","shell.execute_reply.started":"2025-12-03T14:41:27.652207Z","shell.execute_reply":"2025-12-03T14:41:27.657799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex  \n\ndef test_model(model, test_loader, device):\n    metric = JaccardIndex(\n        task=\"multiclass\", \n        num_classes=NUM_CLASSES, \n        ignore_index=255,\n        average=\"none\" \n    ).to(device)\n\n    model.to(device)\n    \n    model.eval()\n    test_loss = 0.0\n    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n            \n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n            \n            loss = total_loss_fn(preds, masks)\n            test_loss += loss.item()\n\n            pred_mask = torch.argmax(preds, dim=1)\n            metric.update(pred_mask, masks)\n    \n    iou_per_class = metric.compute()\n    \n    mIoU = iou_per_class.mean().item()\n    \n    print(\"\\n=== HASIL TESTING ===\")\n    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n    print(\"-\" * 30)\n    \n    class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n    for i, iou in enumerate(iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n        \n    metric.reset()\n    return mIoU, iou_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:41:27.661009Z","iopub.execute_input":"2025-12-03T14:41:27.661254Z","iopub.status.idle":"2025-12-03T14:41:27.675476Z","shell.execute_reply.started":"2025-12-03T14:41:27.661239Z","shell.execute_reply":"2025-12-03T14:41:27.674757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn.functional as F\n\nmodel = CSDNet(num_classes=NUM_CLASSES)\n\nckpt_path = WORKDIR + \"/best_model_csdnet_finetunedpth\"\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\nif isinstance(ckpt, dict):\n    if \"model_state_dict\" in ckpt:\n        ckpt_state = ckpt[\"model_state_dict\"]\n    elif \"state_dict\" in ckpt:\n        ckpt_state = ckpt[\"state_dict\"]\n    elif \"model\" in ckpt:\n        ckpt_state = ckpt[\"model\"]\n    else:\n        ckpt_state = ckpt\nelse:\n    ckpt_state = ckpt\n\ntry:\n    model.load_state_dict(ckpt_state)\n    print(\"Checkpoint loaded with strict=True (perfect match).\")\nexcept Exception as e:\n    print(\"Strict load failed:\", e)\n    model_state = model.state_dict()\n    compatible = {}\n    mismatched = []\n    for k, v in ckpt_state.items():\n        if k in model_state:\n            if v.shape == model_state[k].shape:\n                compatible[k] = v\n            else:\n                mismatched.append((k, v.shape, model_state[k].shape))\n    print(f\"Compatible keys: {len(compatible)} / {len(model_state)}\")\n    if mismatched:\n        print(\"Mismatched params (name, ckpt_shape, model_shape) - top 10 shown:\")\n        for item in mismatched[:10]:\n            print(\" \", item)\n    model_state.update(compatible)\n    model.load_state_dict(model_state)\n    print(\"Loaded compatible weights; mismatched layers left as randomly initialized.\")\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:41:27.676149Z","iopub.execute_input":"2025-12-03T14:41:27.676335Z","iopub.status.idle":"2025-12-03T14:41:28.682752Z","shell.execute_reply.started":"2025-12-03T14:41:27.676320Z","shell.execute_reply":"2025-12-03T14:41:28.682161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n\ntest_mIoU, test_iou_per_class = test_model(model, test_loader, device)\n\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\\n\\n\")\nprint(f\"===================== IoU Per Class ========================\")\nfor i, iou in enumerate(test_iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:41:28.683468Z","iopub.execute_input":"2025-12-03T14:41:28.683770Z","iopub.status.idle":"2025-12-03T14:42:32.175248Z","shell.execute_reply.started":"2025-12-03T14:41:28.683751Z","shell.execute_reply":"2025-12-03T14:42:32.174259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# import os\n# from matplotlib.patches import Patch\n\n# CLASS_NAMES = [\n#     \"Background\",               \n#     \"Water\",                    \n#     \"Building No Damage\",       \n#     \"Building Minor Damage\",    \n#     \"Building Major Damage\",    \n#     \"Building Total Destruction\",\n#     \"Road-Clear\",               \n#     \"Road-Blocked\",             \n#     \"Vehicle\",                  \n#     \"Tree\",                     \n#     \"Pool\"                      \n# ]\n\n# LABEL_COLORS = np.array([\n#     [0, 0, 0],         # Background \n#     [30, 230, 255],    # Water \n#     [184, 115, 117],   # Building No Damage\n#     [216, 255, 0],     # Building Minor Damage\n#     [252, 199, 0],     # Building Major Damage\n#     [255, 0, 0],       # Building Total Destruction\n#     [140, 140, 140],   # Road-Clear\n#     [151, 0, 255],     # Road-Blocked\n#     [255, 0, 246],     # Vehicle \n#     [0, 255, 0],       # Tree\n#     [244, 255, 0]      # Pool\n# ])\n# def decode_segmap(mask):\n#     r = np.zeros_like(mask).astype(np.uint8)\n#     g = np.zeros_like(mask).astype(np.uint8)\n#     b = np.zeros_like(mask).astype(np.uint8)\n    \n#     for l in range(0, len(LABEL_COLORS)):\n#         idx = mask == l\n#         r[idx] = LABEL_COLORS[l, 0]\n#         g[idx] = LABEL_COLORS[l, 1]\n#         b[idx] = LABEL_COLORS[l, 2]\n        \n#     rgb = np.stack([r, g, b], axis=2)\n#     return rgb\n\n# def find_indices_by_filename(dataset, target_ids):\n#     found_indices = []\n#     for target in target_ids:\n#         found = False\n#         for idx, path in enumerate(dataset.image_path):\n#             if str(target) in os.path.basename(path):\n#                 found_indices.append(idx)\n#                 found = True\n#                 break\n#         if not found:\n#             return \n#     return found_indices\n\n# def visualize_specific_images(model, dataset, target_ids, device, processor):\n#     model.eval()\n    \n#     indices = find_indices_by_filename(dataset, target_ids)\n\n#     num_samples = len(indices)\n#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    \n#     if num_samples == 1:\n#         axes = axes.reshape(1, -1)\n\n#     for row_idx, idx in enumerate(indices):\n#         image, mask = dataset[idx] \n        \n#         filename = os.path.basename(dataset.image_path[idx])\n        \n#         inputs = processor(\n#             images=[image], \n#             return_tensors=\"pt\",\n#             do_resize=False, \n#             do_rescale=False\n#         )\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         with torch.no_grad():\n#             outputs = model(**inputs)\n        \n#         target_sizes = [(mask.shape[0], mask.shape[1])]\n#         pred_map = processor.post_process_semantic_segmentation(\n#             outputs, target_sizes=target_sizes\n#         )[0] \n        \n#         img_np = image.permute(1, 2, 0).numpy()\n        \n#         mask_rgb = decode_segmap(mask.numpy())\n#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n        \n#         axes[row_idx, 0].imshow(img_np)\n#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n#         axes[row_idx, 0].axis(\"off\")\n        \n#         axes[row_idx, 1].imshow(mask_rgb)\n#         axes[row_idx, 1].set_title(\"Ground Truth\")\n#         axes[row_idx, 1].axis(\"off\")\n        \n#         axes[row_idx, 2].imshow(pred_rgb)\n#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n#         axes[row_idx, 2].axis(\"off\")\n\n#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n\n#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n    \n#     plt.tight_layout()\n#     plt.subplots_adjust(bottom=0.08) \n#     plt.show()\n\n# target_ids = [\"10794\", \"10801\", \"10807\"]\n\n# visualize_specific_images(model, test_dataset, target_ids, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:42:32.176841Z","iopub.execute_input":"2025-12-03T14:42:32.177117Z","iopub.status.idle":"2025-12-03T14:42:32.183574Z","shell.execute_reply.started":"2025-12-03T14:42:32.177087Z","shell.execute_reply":"2025-12-03T14:42:32.183099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# import matplotlib.pyplot as plt\n\n# test_imgs, test_masks = next(iter(test_loader))\n\n# with torch.no_grad():\n#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n    \n#     outputs = model(inputs)\n    \n#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:42:32.184431Z","iopub.execute_input":"2025-12-03T14:42:32.184831Z","iopub.status.idle":"2025-12-03T14:42:32.211680Z","shell.execute_reply.started":"2025-12-03T14:42:32.184808Z","shell.execute_reply":"2025-12-03T14:42:32.210929Z"}},"outputs":[],"execution_count":null}]}
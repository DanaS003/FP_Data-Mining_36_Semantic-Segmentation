{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13965826,"sourceType":"datasetVersion","datasetId":8902802},{"sourceId":672965,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":509936,"modelId":524601}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:32:30.696691Z","iopub.execute_input":"2025-12-05T03:32:30.697192Z","iopub.status.idle":"2025-12-05T03:33:38.589315Z","shell.execute_reply.started":"2025-12-05T03:32:30.697168Z","shell.execute_reply":"2025-12-05T03:33:38.588593Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:41.390214Z","iopub.execute_input":"2025-12-05T03:33:41.391022Z","iopub.status.idle":"2025-12-05T03:33:46.988527Z","shell.execute_reply.started":"2025-12-05T03:33:41.390973Z","shell.execute_reply":"2025-12-05T03:33:46.987749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{}},{"cell_type":"code","source":"ls /kaggle/input\n\nNUM_CLASSES = 10\nBATCH_SIZE = 16\nEPOCHS = 25\n\nMODEL_NAME = \"best_model_segformer.pth\"\nMODEL_NAME_FINETUNED = \"best_model_segformer_finetuned.pth\"\nDIR_MODEL = \"/kaggle/input/segformer/pytorch/default/1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:50.401546Z","iopub.execute_input":"2025-12-05T03:33:50.402399Z","iopub.status.idle":"2025-12-05T03:33:50.537553Z","shell.execute_reply.started":"2025-12-05T03:33:50.402377Z","shell.execute_reply":"2025-12-05T03:33:50.536833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport re\ndef sort_files_numerically(directory):\n    files = os.listdir(directory)\n    files_sorted = sorted(files, key=lambda x: int(re.search(r'\\d+', x).group()))\n    return [os.path.join(directory, f) for f in files_sorted]\n\nROOT_INP = \"/kaggle/input/indo-flood-segmentation-dataset\"\n\ntrain_image_paths = sort_files_numerically(ROOT_INP+'/train/train-org-img')\ntrain_mask_paths = sort_files_numerically(ROOT_INP+'/train/train-label-img')\n\nval_image_paths = sort_files_numerically(ROOT_INP+'/val/val-org-img')\nval_mask_paths = sort_files_numerically(ROOT_INP+'/val/val-label-img')\n\ntest_image_paths = sort_files_numerically(ROOT_INP+'/test/test-org-img')\ntest_mask_paths = sort_files_numerically(ROOT_INP+'/test/test-label-img')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:52.045809Z","iopub.execute_input":"2025-12-05T03:33:52.046583Z","iopub.status.idle":"2025-12-05T03:33:52.538315Z","shell.execute_reply.started":"2025-12-05T03:33:52.046555Z","shell.execute_reply":"2025-12-05T03:33:52.537727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FloodDataset(Dataset):\n    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.transform = transform\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_path[idx]).convert('RGB')\n        mask = Image.open(self.mask_path[idx]).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        mask = mask.resize(self.image_size, Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        mask = np.clip(mask, 0, 9)\n        mask = torch.from_numpy(mask).long()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:53.630693Z","iopub.execute_input":"2025-12-05T03:33:53.631408Z","iopub.status.idle":"2025-12-05T03:33:53.636454Z","shell.execute_reply.started":"2025-12-05T03:33:53.631384Z","shell.execute_reply":"2025-12-05T03:33:53.635841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"train_test_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = FloodDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\nval_dataset = FloodDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\ntest_dataset = FloodDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:55.337886Z","iopub.execute_input":"2025-12-05T03:33:55.338490Z","iopub.status.idle":"2025-12-05T03:33:55.344223Z","shell.execute_reply.started":"2025-12-05T03:33:55.338464Z","shell.execute_reply":"2025-12-05T03:33:55.343526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_all_unique_masks(dataloader, max_batches=None):\n#     all_unique = set()\n#     for i, (_, masks) in enumerate(dataloader):\n#         all_unique.update(int(u) for u in torch.unique(masks))\n#         if max_batches is not None and i+1 >= max_batches:\n#             break\n#     return sorted(all_unique)\n\n# # print(\"Train unique (scan 200 batches):\", get_all_unique_masks(train_loader, max_batches=200))\n# # print(\"Val unique   (scan all):       \", get_all_unique_masks(val_loader))\n# print(\"Test unique  (scan all):       \", get_all_unique_masks(test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:06:41.370532Z","iopub.execute_input":"2025-12-04T09:06:41.370721Z","iopub.status.idle":"2025-12-04T09:06:41.387635Z","shell.execute_reply.started":"2025-12-04T09:06:41.370706Z","shell.execute_reply":"2025-12-04T09:06:41.386969Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport segmentation_models_pytorch as smp\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = smp.Segformer(\n    encoder_name=\"mit_b1\",       \n    encoder_weights=\"imagenet\",  \n    in_channels=3,\n    classes=NUM_CLASSES,                  \n    activation=None              \n)\n\nCKPT_PATH = DIR_NAME + MODEL_NAME\ncheckpoint_trained = torch.load(CKPT_PATH, map_location=DEVICE)\n\nif 'state_dict' in checkpoint:\n    state_dict = checkpoint['state_dict']\nelif 'model_state_dict' in checkpoint:\n    state_dict = checkpoint['model_state_dict']\nelse:\n    state_dict = checkpoint\n\nnew_state_dict = {}\nfor key, value in state_dict.items():\n    if key.startswith(\"segmentation_head\"):\n        print(f\"Skipping weight: {key} (Shape mismatch expected)\")\n        continue\n    new_state_dict[key] = value\n\nmsg = model.load_state_dict(new_state_dict, strict=False)\n\nprint(\"\\nStatus Load Model:\")\nprint(msg) \n\nmodel.to(DEVICE)\nprint(\"Model SegFormer berhasil di-load dan siap untuk Fine-Tuning!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:33:58.999631Z","iopub.execute_input":"2025-12-05T03:33:59.000394Z","iopub.status.idle":"2025-12-05T03:34:07.831622Z","shell.execute_reply.started":"2025-12-05T03:33:59.000368Z","shell.execute_reply":"2025-12-05T03:34:07.830984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train n Eval","metadata":{}},{"cell_type":"code","source":"print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\nprint(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"DEVICE USED : {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:13:33.945643Z","iopub.execute_input":"2025-12-04T09:13:33.945954Z","iopub.status.idle":"2025-12-04T09:13:33.950890Z","shell.execute_reply.started":"2025-12-04T09:13:33.945906Z","shell.execute_reply":"2025-12-04T09:13:33.949977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JointLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ce = nn.CrossEntropyLoss()\n        self.dice = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n\n    def forward(self, logits, targets):\n        loss_ce = self.ce(logits, targets)\n        loss_dice = self.dice(logits, targets)\n        return loss_ce + loss_dice\n\ntotal_loss_fn = JointLoss()\n\nencoder_params = []\ndecoder_params = []\n\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    \n    if name.startswith(\"encoder\"):\n        encoder_params.append(param)\n    else:\n        decoder_params.append(param)\n\noptimizer = AdamW(\n    [\n    {'params': encoder_params, 'lr': 6e-4},  \n    {'params': decoder_params, 'lr': 9e-3}   \n], \n    # model.parameters(),\n    weight_decay=1e-2)    \n    # lr=6e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:34:07.832638Z","iopub.execute_input":"2025-12-05T03:34:07.832912Z","iopub.status.idle":"2025-12-05T03:34:07.840021Z","shell.execute_reply.started":"2025-12-05T03:34:07.832892Z","shell.execute_reply":"2025-12-05T03:34:07.839325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics import JaccardIndex\nfrom tqdm.auto import tqdm\n\n# EPOCHS = 1\n\nscheduler = CosineAnnealingLR(\n    optimizer,\n    T_max=EPOCHS,\n    eta_min=1e-6 \n)\n\nmetric_val   = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(device)\n\nbest_mIoU = 0.0\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_miou\": []\n}\n\nfor epoch in range(EPOCHS):\n\n    model.train()\n    train_loss = 0\n\n    if hasattr(model, 'mod1_detection'): \n        model.mod1_detection.eval()\n        \n    for images, masks in tqdm(train_loader, desc=f\"Train {epoch+1}/{EPOCHS}\"):\n        images, masks = images.to(device), masks.to(device)\n\n        optimizer.zero_grad()\n        preds = model(images)\n        loss = total_loss_fn(preds, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        del preds, loss\n        torch.cuda.empty_cache()\n\n    train_loss /= len(train_loader)\n\n    print(f\"ðŸŽ¯ Train Loss: {train_loss:.4f}\")\n    history[\"train_loss\"].append(train_loss)\n\n    model.eval()\n    val_loss = 0\n    metric_val.reset()\n\n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=f\"Val {epoch+1}/{EPOCHS}\"):\n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n            loss = total_loss_fn(preds, masks)\n            val_loss += loss.item()\n\n            pred_mask = preds.argmax(dim=1)\n            metric_val.update(pred_mask, masks)\n\n            del preds, loss\n            torch.cuda.empty_cache()\n\n    val_loss /= len(val_loader)\n    iou_val_per_class = metric_val.compute()\n    mIoU_val = iou_val_per_class.mean().item()\n\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_miou\"].append(mIoU_val)\n\n    print(f\"\"\"\n============================== Epoch: {epoch+1}/{EPOCHS}\nTrain Loss: {train_loss:.4f}\nVal Loss:   {val_loss:.4f}   | Val mIoU: {mIoU_val:.4f}\n==============================\n\"\"\")\n\n\n    scheduler.step(val_loss)\n\n    if best_mIoU < mIoU_val:\n        best_mIoU = mIoU_val\n        torch.save(model.state_dict(), \"best_model_segformer_finetuned.pth\")\n        print(\"Model disimpan (best so far).\")\n            \nprint(\"Training Selesai!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:13:38.154312Z","iopub.execute_input":"2025-12-04T09:13:38.154893Z","iopub.status.idle":"2025-12-04T10:26:25.550311Z","shell.execute_reply.started":"2025-12-04T09:13:38.154870Z","shell.execute_reply":"2025-12-04T10:26:25.549180Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# print(os.getcwd())\n# print(os.listdir())\n# from IPython.display import FileLink\n\n# FileLink(\"best_model_csdnet.pth\")\n# # FileLink(\"history_csdnet.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:13:27.080200Z","iopub.status.idle":"2025-12-04T09:13:27.080464Z","shell.execute_reply.started":"2025-12-04T09:13:27.080352Z","shell.execute_reply":"2025-12-04T09:13:27.080362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nWORKDIR=\"/kaggle/working\"\noutput_path = WORKDIR+\"/history_segformer_finetuned.json\"\n\nwith open(output_path, \"w\") as f:\n    json.dump(history, f, indent=4)\n\nprint(\"File saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:27:26.121944Z","iopub.status.idle":"2025-12-04T10:27:26.122219Z","shell.execute_reply.started":"2025-12-04T10:27:26.122091Z","shell.execute_reply":"2025-12-04T10:27:26.122103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex  \n\ndef test_model(model, test_loader, device):\n    metric = JaccardIndex(\n        task=\"multiclass\", \n        num_classes=NUM_CLASSES, \n        ignore_index=255,\n        average=\"none\" \n    ).to(device)\n\n    model.to(device)\n    \n    model.eval()\n    test_loss = 0.0\n    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n            \n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n            \n            loss = total_loss_fn(preds, masks)\n            test_loss += loss.item()\n\n            pred_mask = torch.argmax(preds, dim=1)\n            metric.update(pred_mask, masks)\n    \n    iou_per_class = metric.compute()\n    \n    mIoU = iou_per_class.mean().item()\n    \n    print(\"\\n=== HASIL TESTING ===\")\n    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n    print(\"-\" * 30)\n    \n    class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n    for i, iou in enumerate(iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n        \n    metric.reset()\n    return mIoU, iou_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:34:13.683623Z","iopub.execute_input":"2025-12-05T03:34:13.684193Z","iopub.status.idle":"2025-12-05T03:34:17.905587Z","shell.execute_reply.started":"2025-12-05T03:34:13.684168Z","shell.execute_reply":"2025-12-05T03:34:17.904995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn.functional as F\n\nmodel = smp.Segformer(\n    encoder_name=\"mit_b1\",       \n    encoder_weights=\"imagenet\",  \n    in_channels=3,\n    classes=NUM_CLASSES                  \n    activation=None              \n)\n# /kaggle/input/segformer/pytorch/default/1\n# _DIR = \"/kaggle/working/\"\n\nckpt_path = WORKDIR + MODEL_NAME_FINETUNED\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\nif isinstance(ckpt, dict):\n    if \"model_state_dict\" in ckpt:\n        ckpt_state = ckpt[\"model_state_dict\"]\n    elif \"state_dict\" in ckpt:\n        ckpt_state = ckpt[\"state_dict\"]\n    elif \"model\" in ckpt:\n        ckpt_state = ckpt[\"model\"]\n    else:\n        ckpt_state = ckpt\nelse:\n    ckpt_state = ckpt\n\ntry:\n    model.load_state_dict(ckpt_state)\n    print(\"Checkpoint loaded with strict=True (perfect match).\")\nexcept Exception as e:\n    print(\"Strict load failed:\", e)\n    model_state = model.state_dict()\n    compatible = {}\n    mismatched = []\n    for k, v in ckpt_state.items():\n        if k in model_state:\n            if v.shape == model_state[k].shape:\n                compatible[k] = v\n            else:\n                mismatched.append((k, v.shape, model_state[k].shape))\n    print(f\"Compatible keys: {len(compatible)} / {len(model_state)}\")\n    if mismatched:\n        print(\"Mismatched params (name, ckpt_shape, model_shape) - top 10 shown:\")\n        for item in mismatched[:10]:\n            print(\" \", item)\n    model_state.update(compatible)\n    model.load_state_dict(model_state)\n    print(\"Loaded compatible weights; mismatched layers left as randomly initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:38:07.994432Z","iopub.execute_input":"2025-12-05T03:38:07.995173Z","iopub.status.idle":"2025-12-05T03:38:08.489644Z","shell.execute_reply.started":"2025-12-05T03:38:07.995144Z","shell.execute_reply":"2025-12-05T03:38:08.488950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n               \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n\ntest_mIoU, test_iou_per_class = test_model(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:38:11.561559Z","iopub.execute_input":"2025-12-05T03:38:11.562278Z","iopub.status.idle":"2025-12-05T03:38:11.590966Z","shell.execute_reply.started":"2025-12-05T03:38:11.562243Z","shell.execute_reply":"2025-12-05T03:38:11.590049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# import os\n# from matplotlib.patches import Patch\n\n# CLASS_NAMES = [\n#     \"Background\",               \n#     \"Water\",                    \n#     \"Building No Damage\",       \n#     \"Building Minor Damage\",    \n#     \"Building Major Damage\",    \n#     \"Building Total Destruction\",\n#     \"Road-Clear\",               \n#     \"Road-Blocked\",             \n#     \"Vehicle\",                  \n#     \"Tree\",                     \n#     \"Pool\"                      \n# ]\n\n# LABEL_COLORS = np.array([\n#     [0, 0, 0],         # Background \n#     [30, 230, 255],    # Water \n#     [184, 115, 117],   # Building No Damage\n#     [216, 255, 0],     # Building Minor Damage\n#     [252, 199, 0],     # Building Major Damage\n#     [255, 0, 0],       # Building Total Destruction\n#     [140, 140, 140],   # Road-Clear\n#     [151, 0, 255],     # Road-Blocked\n#     [255, 0, 246],     # Vehicle \n#     [0, 255, 0],       # Tree\n#     [244, 255, 0]      # Pool\n# ])\n# def decode_segmap(mask):\n#     r = np.zeros_like(mask).astype(np.uint8)\n#     g = np.zeros_like(mask).astype(np.uint8)\n#     b = np.zeros_like(mask).astype(np.uint8)\n    \n#     for l in range(0, len(LABEL_COLORS)):\n#         idx = mask == l\n#         r[idx] = LABEL_COLORS[l, 0]\n#         g[idx] = LABEL_COLORS[l, 1]\n#         b[idx] = LABEL_COLORS[l, 2]\n        \n#     rgb = np.stack([r, g, b], axis=2)\n#     return rgb\n\n# def find_indices_by_filename(dataset, target_ids):\n#     found_indices = []\n#     for target in target_ids:\n#         found = False\n#         for idx, path in enumerate(dataset.image_path):\n#             if str(target) in os.path.basename(path):\n#                 found_indices.append(idx)\n#                 found = True\n#                 break\n#         if not found:\n#             return \n#     return found_indices\n\n# def visualize_specific_images(model, dataset, target_ids, device, processor):\n#     model.eval()\n    \n#     indices = find_indices_by_filename(dataset, target_ids)\n\n#     num_samples = len(indices)\n#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    \n#     if num_samples == 1:\n#         axes = axes.reshape(1, -1)\n\n#     for row_idx, idx in enumerate(indices):\n#         image, mask = dataset[idx] \n        \n#         filename = os.path.basename(dataset.image_path[idx])\n        \n#         inputs = processor(\n#             images=[image], \n#             return_tensors=\"pt\",\n#             do_resize=False, \n#             do_rescale=False\n#         )\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         with torch.no_grad():\n#             outputs = model(**inputs)\n        \n#         target_sizes = [(mask.shape[0], mask.shape[1])]\n#         pred_map = processor.post_process_semantic_segmentation(\n#             outputs, target_sizes=target_sizes\n#         )[0] \n        \n#         img_np = image.permute(1, 2, 0).numpy()\n        \n#         mask_rgb = decode_segmap(mask.numpy())\n#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n        \n#         axes[row_idx, 0].imshow(img_np)\n#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n#         axes[row_idx, 0].axis(\"off\")\n        \n#         axes[row_idx, 1].imshow(mask_rgb)\n#         axes[row_idx, 1].set_title(\"Ground Truth\")\n#         axes[row_idx, 1].axis(\"off\")\n        \n#         axes[row_idx, 2].imshow(pred_rgb)\n#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n#         axes[row_idx, 2].axis(\"off\")\n\n#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n\n#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n    \n#     plt.tight_layout()\n#     plt.subplots_adjust(bottom=0.08) \n#     plt.show()\n\n# target_ids = [\"10794\", \"10801\", \"10807\"]\n\n# visualize_specific_images(model, test_dataset, target_ids, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:13:27.087866Z","iopub.status.idle":"2025-12-04T09:13:27.088183Z","shell.execute_reply.started":"2025-12-04T09:13:27.088073Z","shell.execute_reply":"2025-12-04T09:13:27.088084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# import matplotlib.pyplot as plt\n\n# test_imgs, test_masks = next(iter(test_loader))\n\n# with torch.no_grad():\n#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n    \n#     outputs = model(inputs)\n    \n#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:13:27.089133Z","iopub.status.idle":"2025-12-04T09:13:27.089364Z","shell.execute_reply.started":"2025-12-04T09:13:27.089248Z","shell.execute_reply":"2025-12-04T09:13:27.089260Z"}},"outputs":[],"execution_count":null}]}
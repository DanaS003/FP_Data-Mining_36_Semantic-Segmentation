{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"import numpy\nimport scipy\nprint(f\"Numpy version: {numpy.__version__}\")\nprint(f\"Scipy version: {scipy.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:11.906911Z","iopub.execute_input":"2025-12-04T03:12:11.907259Z","iopub.status.idle":"2025-12-04T03:12:11.924509Z","shell.execute_reply.started":"2025-12-04T03:12:11.907235Z","shell.execute_reply":"2025-12-04T03:12:11.923746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:11.925584Z","iopub.execute_input":"2025-12-04T03:12:11.925793Z","iopub.status.idle":"2025-12-04T03:12:17.238613Z","shell.execute_reply.started":"2025-12-04T03:12:11.925777Z","shell.execute_reply":"2025-12-04T03:12:17.238044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{}},{"cell_type":"code","source":"ls /kaggle/input\n\nNUM_CLASSES = 10\nBATCH_SIZE = 16\nEPOCHS = 25\n\nMODEL_NAME = \"best_model_pspnet.pth\"\nMODEL_NAME_FINETUNED = \"best_model_pspnet_finetuned.pth\"\nDIR_MODEL = \"/kaggle/input/pspnet/pytorch/default/1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:17.239298Z","iopub.execute_input":"2025-12-04T03:12:17.239602Z","iopub.status.idle":"2025-12-04T03:12:17.371242Z","shell.execute_reply.started":"2025-12-04T03:12:17.239584Z","shell.execute_reply":"2025-12-04T03:12:17.370572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport re\ndef sort_files_numerically(directory):\n    files = os.listdir(directory)\n    files_sorted = sorted(files, key=lambda x: int(re.search(r'\\d+', x).group()))\n    return [os.path.join(directory, f) for f in files_sorted]\n\nROOT_INP = \"/kaggle/input/indo-flood-segmentation-dataset\"\n\ntrain_image_paths = sort_files_numerically(ROOT_INP+'/train/train-org-img')\ntrain_mask_paths = sort_files_numerically(ROOT_INP+'/train/train-label-img')\n\nval_image_paths = sort_files_numerically(ROOT_INP+'/val/val-org-img')\nval_mask_paths = sort_files_numerically(ROOT_INP+'/val/val-label-img')\n\ntest_image_paths = sort_files_numerically(ROOT_INP+'/test/test-org-img')\ntest_mask_paths = sort_files_numerically(ROOT_INP+'/test/test-label-img')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:17.373145Z","iopub.execute_input":"2025-12-04T03:12:17.373603Z","iopub.status.idle":"2025-12-04T03:12:17.542843Z","shell.execute_reply.started":"2025-12-04T03:12:17.373578Z","shell.execute_reply":"2025-12-04T03:12:17.542281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FloodDataset(Dataset):\n    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.transform = transform\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_path[idx]).convert('RGB')\n        mask = Image.open(self.mask_path[idx]).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        mask = mask.resize(self.image_size, Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        mask = np.clip(mask, 0, 9)\n        mask = torch.from_numpy(mask).long()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:17.543539Z","iopub.execute_input":"2025-12-04T03:12:17.543747Z","iopub.status.idle":"2025-12-04T03:12:17.549299Z","shell.execute_reply.started":"2025-12-04T03:12:17.543731Z","shell.execute_reply":"2025-12-04T03:12:17.548625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"train_test_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = FloodDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\nval_dataset = FloodDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\ntest_dataset = FloodDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:17.549974Z","iopub.execute_input":"2025-12-04T03:12:17.550356Z","iopub.status.idle":"2025-12-04T03:12:17.564205Z","shell.execute_reply.started":"2025-12-04T03:12:17.550337Z","shell.execute_reply":"2025-12-04T03:12:17.563568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass PSPNet(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, backbone_name=\"efficientnet_b3\"):\n        super().__init__()\n\n        self.encoder = timm.create_model(backbone_name, pretrained=True, features_only=True)\n\n        encoder_channels = self.encoder.feature_info.channels()\n        bottleneck_dim = encoder_channels[-1]\n\n        self.ppm = nn.ModuleList([\n            nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(bottleneck_dim, 256, kernel_size=1),\n                nn.ReLU(inplace=True)\n            )\n            for scale in [1, 2, 3, 6]\n        ])\n\n        self.final = nn.Sequential(\n            nn.Conv2d(bottleneck_dim + 4 * 256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1)\n        )\n\n    def forward(self, x):\n        feats = self.encoder(x)\n        x = feats[-1]\n\n        ppm_outs = [x]\n        for pool in self.ppm:\n            pooled = pool(x)\n            pooled = nn.functional.interpolate(\n                pooled, size=x.shape[2:], mode=\"bilinear\", align_corners=False\n            )\n            ppm_outs.append(pooled)\n\n        x = torch.cat(ppm_outs, dim=1)\n        x = self.final(x)\n        x = nn.functional.interpolate(x, scale_factor=8, mode=\"bilinear\", align_corners=False)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:17.564922Z","iopub.execute_input":"2025-12-04T03:12:17.565610Z","iopub.status.idle":"2025-12-04T03:12:20.963901Z","shell.execute_reply.started":"2025-12-04T03:12:17.565594Z","shell.execute_reply":"2025-12-04T03:12:20.963322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"DEVICE {device}\")\n\nmodel = PSPNet(num_classes=NUM_CLASSES)\n\nif os.path.exists(WEIGHTS_PATH):\n    print(f\"Loading weights from: {WEIGHTS_PATH}\")\n    checkpoint = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n    \n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n\n    new_state_dict = {}\n    \n    for key, value in state_dict.items():\n        if \"final.3\" in key:\n            print(f\"Skipping weight: {key} (Shape mismatch due to different class count)\")\n            continue\n            \n        new_state_dict[key] = value\n    msg = model.load_state_dict(new_state_dict, strict=False)\n    \n    print(\"\\nStatus Load Model:\")\n    print(msg)\nelse:\n    print(f\"Error: File path {WEIGHTS_PATH} tidak ditemukan.\")\n\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\nprint(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:20.964686Z","iopub.execute_input":"2025-12-04T03:12:20.964910Z","iopub.status.idle":"2025-12-04T03:12:20.969415Z","shell.execute_reply.started":"2025-12-04T03:12:20.964892Z","shell.execute_reply":"2025-12-04T03:12:20.968536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_params = list(model.encoder.parameters())\ndecoder_params = list(model.ppm.parameters()) + list(model.final.parameters())\n\noptimizer = torch.optim.AdamW([\n    {\"params\": encoder_params, \"lr\": 6e-4},\n    {\"params\": decoder_params, \"lr\": 9e-3},\n], weight_decay=1e-2)\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, logits, target):\n        logits = torch.softmax(logits, dim=1)\n        target_1hot = torch.nn.functional.one_hot(target, num_classes=logits.size(1)).permute(0,3,1,2)\n\n        intersection = (logits * target_1hot).sum(dim=(2,3))\n        union = logits.sum(dim=(2,3)) + target_1hot.sum(dim=(2,3))\n\n        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n        return 1 - dice.mean()\n\n\nce_loss = nn.CrossEntropyLoss()\ndice_loss = DiceLoss()\n\ndef total_loss_fn(pred, target):\n    return ce_loss(pred, target.long()) + dice_loss(pred, target.long())\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=100, \n    eta_min=1e-6\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:12:20.970926Z","iopub.execute_input":"2025-12-04T03:12:20.971188Z","iopub.status.idle":"2025-12-04T03:12:23.698761Z","shell.execute_reply.started":"2025-12-04T03:12:20.971166Z","shell.execute_reply":"2025-12-04T03:12:23.698216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex\n\nmetric_val = JaccardIndex(\n    task=\"multiclass\",\n    num_classes=NUM_CLASSES,\n    average=None\n).to(device)\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_miou\": []\n}\n\nbest_loss = float(\"inf\")\n# patience_counter = 0\n\nfor epoch in range(EPOCHS):\n\n    model.train()\n    train_loss = 0.0\n\n    for images, masks in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{EPOCHS}\"):\n\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n\n        preds = model(images)\n\n        preds = torch.nn.functional.interpolate(\n                    preds, \n                    size=masks.shape[-2:], \n                    mode=\"bilinear\", \n                    align_corners=False\n                )\n\n        loss = total_loss_fn(preds, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    \n    print(f\"ðŸŽ¯ Train Loss: {train_loss:.4f}\")\n    history[\"train_loss\"].append(train_loss)\n\n    model.eval()\n    val_loss = 0.0\n    metric_val.reset()\n\n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}/{EPOCHS}\"):\n\n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n\n            preds = torch.nn.functional.interpolate(\n                        preds, \n                        size=masks.shape[-2:], \n                        mode=\"bilinear\", \n                        align_corners=False\n                    )\n            loss = total_loss_fn(preds, masks)\n            val_loss += loss.item()\n\n            pred_mask = torch.argmax(preds, dim=1)\n            metric_val.update(pred_mask, masks)\n\n    val_loss /= len(val_loader)\n    iou_val_per_class = metric_val.compute()\n    mIoU_val = iou_val_per_class.mean().item()\n\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_miou\"].append(mIoU_val)\n\n    # scheduler.step()\n\n    print(\"\\n=============================\")\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f} | Val mIoU: {mIoU_val:.4f}\")\n    print(\"=============================\\n\")\n\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), \"best_model_pspnet_finetuned.pth\")\n        print(\"Model disimpan (best so far).\")\nprint(\"Training Selesai!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:40:23.704933Z","iopub.execute_input":"2025-12-04T03:40:23.705492Z","iopub.status.idle":"2025-12-04T04:05:48.463079Z","shell.execute_reply.started":"2025-12-04T03:40:23.705469Z","shell.execute_reply":"2025-12-04T04:05:48.462254Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nWORKDIR = \"/kaggle/working\"\n\noutput_path = WORKDIR+\"/history_pspnet_finetuned.json\"\n\nwith open(output_path, \"w\") as f:\n    json.dump(history, f, indent=4)\n\nprint(\"File saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:38:39.478377Z","iopub.execute_input":"2025-12-04T03:38:39.479378Z","iopub.status.idle":"2025-12-04T03:38:39.485863Z","shell.execute_reply.started":"2025-12-04T03:38:39.479334Z","shell.execute_reply":"2025-12-04T03:38:39.485124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex  \n\ndef test_model(model, test_loader, device):\n    metric = JaccardIndex(\n        task=\"multiclass\", \n        num_classes=NUM_CLASSES, \n        ignore_index=255,\n        average=\"none\" \n    ).to(device)\n\n    model.to(device)\n    \n    model.eval()\n    test_loss = 0.0\n    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n            \n            images, masks = images.to(device), masks.to(device)\n\n            preds = model(images)\n\n            preds = torch.nn.functional.interpolate(\n                        preds, \n                        size=masks.shape[-2:], \n                        mode=\"bilinear\", \n                        align_corners=False\n                    )\n            loss = total_loss_fn(preds, masks)\n            test_loss += loss.item()\n\n            pred_mask = torch.argmax(preds, dim=1)\n            metric.update(pred_mask, masks)\n    \n    iou_per_class = metric.compute()\n    \n    mIoU = iou_per_class.mean().item()\n    \n    print(\"\\n=== HASIL TESTING ===\")\n    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n    print(\"-\" * 30)\n    \n    class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n    for i, iou in enumerate(iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n        \n    metric.reset()\n    return mIoU, iou_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T04:15:38.418045Z","iopub.execute_input":"2025-12-04T04:15:38.418576Z","iopub.status.idle":"2025-12-04T04:15:38.428258Z","shell.execute_reply.started":"2025-12-04T04:15:38.418538Z","shell.execute_reply":"2025-12-04T04:15:38.427609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n\nmodel = PSPNet(num_classes=NUM_CLASSES).to(device)\n\nckpt_path = WORKDIR + \"/best_model_pspnet_finetuned.pth\"\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\nif isinstance(ckpt, dict):\n    if \"model_state_dict\" in ckpt:\n        ckpt_state = ckpt[\"model_state_dict\"]\n    elif \"state_dict\" in ckpt:\n        ckpt_state = ckpt[\"state_dict\"]\n    elif \"model\" in ckpt:\n        ckpt_state = ckpt[\"model\"]\n    else:\n        ckpt_state = ckpt\nelse:\n    ckpt_state = ckpt\n\ntry:\n    model.load_state_dict(ckpt_state)\n    print(\"Loaded checkpoint with strict=True (perfect match).\")\nexcept RuntimeError as e:\n    print(\"Strict load failed (mismatch). Error:\", e)\n    model_state = model.state_dict()\n    compatible = {}\n    mismatched = []\n    for k, v in ckpt_state.items():\n        if k in model_state:\n            if v.shape == model_state[k].shape:\n                compatible[k] = v\n            else:\n                mismatched.append((k, v.shape, model_state[k].shape))\n    print(f\"Compatible keys: {len(compatible)} / {len(model_state)}\")\n    if mismatched:\n        print(\"Mismatched keys (name, ckpt_shape, model_shape):\")\n        for item in mismatched:\n            print(\" \", item)\n\n    model_state.update(compatible)\n    model.load_state_dict(model_state)\n    print(\"Loaded compatible weights; mismatched layers left as randomly initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T04:15:41.274468Z","iopub.execute_input":"2025-12-04T04:15:41.275221Z","iopub.status.idle":"2025-12-04T04:16:12.562720Z","shell.execute_reply.started":"2025-12-04T04:15:41.275196Z","shell.execute_reply":"2025-12-04T04:16:12.562070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_mIoU, test_iou_per_class = test_model(model, test_loader, device)\n\n\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\")\nprint(f\"===================== Hasil Testing mIoU {test_mIoU} ========================\\n\\n\\n\")\nprint(f\"===================== IoU Per Class ========================\")\nfor i, iou in enumerate(test_iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T04:16:12.564012Z","iopub.execute_input":"2025-12-04T04:16:12.564289Z","iopub.status.idle":"2025-12-04T04:17:14.903881Z","shell.execute_reply.started":"2025-12-04T04:16:12.564269Z","shell.execute_reply":"2025-12-04T04:17:14.903049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualisasi Sample Test","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# import os\n# from matplotlib.patches import Patch\n\n# CLASS_NAMES = [\n#     \"Background\",               \n#     \"Water\",                    \n#     \"Building No Damage\",       \n#     \"Building Minor Damage\",    \n#     \"Building Major Damage\",    \n#     \"Building Total Destruction\",\n#     \"Road-Clear\",               \n#     \"Road-Blocked\",             \n#     \"Vehicle\",                  \n#     \"Tree\",                     \n#     \"Pool\"                      \n# ]\n\n# LABEL_COLORS = np.array([\n#     [0, 0, 0],         # Background \n#     [30, 230, 255],    # Water \n#     [184, 115, 117],   # Building No Damage\n#     [216, 255, 0],     # Building Minor Damage\n#     [252, 199, 0],     # Building Major Damage\n#     [255, 0, 0],       # Building Total Destruction\n#     [140, 140, 140],   # Road-Clear\n#     [151, 0, 255],     # Road-Blocked\n#     [255, 0, 246],     # Vehicle \n#     [0, 255, 0],       # Tree\n#     [244, 255, 0]      # Pool\n# ])\n# def decode_segmap(mask):\n#     r = np.zeros_like(mask).astype(np.uint8)\n#     g = np.zeros_like(mask).astype(np.uint8)\n#     b = np.zeros_like(mask).astype(np.uint8)\n    \n#     for l in range(0, len(LABEL_COLORS)):\n#         idx = mask == l\n#         r[idx] = LABEL_COLORS[l, 0]\n#         g[idx] = LABEL_COLORS[l, 1]\n#         b[idx] = LABEL_COLORS[l, 2]\n        \n#     rgb = np.stack([r, g, b], axis=2)\n#     return rgb\n\n# def find_indices_by_filename(dataset, target_ids):\n#     found_indices = []\n#     for target in target_ids:\n#         found = False\n#         for idx, path in enumerate(dataset.image_path):\n#             if str(target) in os.path.basename(path):\n#                 found_indices.append(idx)\n#                 found = True\n#                 break\n#         if not found:\n#             return \n#     return found_indices\n\n# def visualize_specific_images(model, dataset, target_ids, device, processor):\n#     model.eval()\n    \n#     indices = find_indices_by_filename(dataset, target_ids)\n\n#     num_samples = len(indices)\n#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    \n#     if num_samples == 1:\n#         axes = axes.reshape(1, -1)\n\n#     for row_idx, idx in enumerate(indices):\n#         image, mask = dataset[idx] \n        \n#         filename = os.path.basename(dataset.image_path[idx])\n        \n#         inputs = processor(\n#             images=[image], \n#             return_tensors=\"pt\",\n#             do_resize=False, \n#             do_rescale=False\n#         )\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         with torch.no_grad():\n#             outputs = model(**inputs)\n        \n#         target_sizes = [(mask.shape[0], mask.shape[1])]\n#         pred_map = processor.post_process_semantic_segmentation(\n#             outputs, target_sizes=target_sizes\n#         )[0] \n        \n#         img_np = image.permute(1, 2, 0).numpy()\n        \n#         mask_rgb = decode_segmap(mask.numpy())\n#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n        \n#         axes[row_idx, 0].imshow(img_np)\n#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n#         axes[row_idx, 0].axis(\"off\")\n        \n#         axes[row_idx, 1].imshow(mask_rgb)\n#         axes[row_idx, 1].set_title(\"Ground Truth\")\n#         axes[row_idx, 1].axis(\"off\")\n        \n#         axes[row_idx, 2].imshow(pred_rgb)\n#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n#         axes[row_idx, 2].axis(\"off\")\n\n#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n\n#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n    \n#     plt.tight_layout()\n#     plt.subplots_adjust(bottom=0.08) \n#     plt.show()\n\n# target_ids = [\"10794\", \"10801\", \"10807\"]\n\n# visualize_specific_images(model, test_dataset, target_ids, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:39:59.280056Z","iopub.execute_input":"2025-12-04T03:39:59.280320Z","iopub.status.idle":"2025-12-04T03:39:59.286252Z","shell.execute_reply.started":"2025-12-04T03:39:59.280289Z","shell.execute_reply":"2025-12-04T03:39:59.285636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# import matplotlib.pyplot as plt\n\n# test_imgs, test_masks = next(iter(test_loader))\n\n# with torch.no_grad():\n#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n    \n#     outputs = model(inputs)\n    \n#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:39:59.287630Z","iopub.execute_input":"2025-12-04T03:39:59.287841Z","iopub.status.idle":"2025-12-04T03:39:59.304960Z","shell.execute_reply.started":"2025-12-04T03:39:59.287827Z","shell.execute_reply":"2025-12-04T03:39:59.304275Z"}},"outputs":[],"execution_count":null}]}
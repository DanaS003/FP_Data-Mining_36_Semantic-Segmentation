{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12690938,"sourceType":"datasetVersion","datasetId":8020112},{"sourceId":674898,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":511574,"modelId":526241}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y transformers accelerate tokenizers numpy\n\n!pip install numpy==1.26.4\n!pip install -U transformers accelerate tokenizers evaluate torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:29.447069Z","iopub.execute_input":"2025-12-07T02:59:29.448022Z","iopub.status.idle":"2025-12-07T02:59:49.073717Z","shell.execute_reply.started":"2025-12-07T02:59:29.447983Z","shell.execute_reply":"2025-12-07T02:59:49.072951Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.3\nUninstalling transformers-4.57.3:\n  Successfully uninstalled transformers-4.57.3\nFound existing installation: accelerate 1.12.0\nUninstalling accelerate-1.12.0:\n  Successfully uninstalled accelerate-1.12.0\nFound existing installation: tokenizers 0.22.1\nUninstalling tokenizers-0.22.1:\n  Successfully uninstalled tokenizers-0.22.1\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nCollecting numpy==1.26.4\n  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\nInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires tokenizers, which is not installed.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\npeft 0.16.0 requires accelerate>=0.21.0, which is not installed.\npeft 0.16.0 requires transformers, which is not installed.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nCollecting transformers\n  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\nCollecting accelerate\n  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nCollecting tokenizers\n  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.8.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.15.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nUsing cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\nUsing cached accelerate-1.12.0-py3-none-any.whl (380 kB)\nUsing cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nInstalling collected packages: tokenizers, transformers, accelerate\nSuccessfully installed accelerate-1.12.0 tokenizers-0.22.1 transformers-4.57.3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy\nimport scipy\nprint(f\"Numpy version: {numpy.__version__}\")\nprint(f\"Scipy version: {scipy.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.075207Z","iopub.execute_input":"2025-12-07T02:59:49.075836Z","iopub.status.idle":"2025-12-07T02:59:49.080700Z","shell.execute_reply.started":"2025-12-07T02:59:49.075810Z","shell.execute_reply":"2025-12-07T02:59:49.079839Z"}},"outputs":[{"name":"stdout","text":"Numpy version: 1.26.4\nScipy version: 1.15.3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.081456Z","iopub.execute_input":"2025-12-07T02:59:49.081675Z","iopub.status.idle":"2025-12-07T02:59:49.097679Z","shell.execute_reply.started":"2025-12-07T02:59:49.081649Z","shell.execute_reply":"2025-12-07T02:59:49.097022Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input\n\nNUM_CLASSES = 10\nBATCH_SIZE = 2\nEPOCHS = 25\n\nMODEL_NAME = \"best_model_mask2former.pth\"\nMODEL_NAME_FINETUNED = \"best_model_mask2former_finetuned.pth\"\nDIR_MODEL = \"/kaggle/input/mask2former/pytorch/default/1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.099031Z","iopub.execute_input":"2025-12-07T02:59:49.099262Z","iopub.status.idle":"2025-12-07T02:59:49.243141Z","shell.execute_reply.started":"2025-12-07T02:59:49.099245Z","shell.execute_reply":"2025-12-07T02:59:49.242434Z"}},"outputs":[{"name":"stdout","text":"indo-flood-segmentation-dataset  mask2former\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import glob\nimport re\ndef sort_files_numerically(directory):\n    files = os.listdir(directory)\n    files_sorted = sorted(files, key=lambda x: int(re.search(r'\\d+', x).group()))\n    return [os.path.join(directory, f) for f in files_sorted]\n\nROOT_INP = \"/kaggle/input/indo-flood-segmentation-dataset\"\n\ntrain_image_paths = sort_files_numerically(ROOT_INP+'/train/train-org-img')\ntrain_mask_paths = sort_files_numerically(ROOT_INP+'/train/train-label-img')\n\nval_image_paths = sort_files_numerically(ROOT_INP+'/val/val-org-img')\nval_mask_paths = sort_files_numerically(ROOT_INP+'/val/val-label-img')\n\ntest_image_paths = sort_files_numerically(ROOT_INP+'/test/test-org-img')\ntest_mask_paths = sort_files_numerically(ROOT_INP+'/test/test-label-img')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.244096Z","iopub.execute_input":"2025-12-07T02:59:49.244294Z","iopub.status.idle":"2025-12-07T02:59:49.324221Z","shell.execute_reply.started":"2025-12-07T02:59:49.244274Z","shell.execute_reply":"2025-12-07T02:59:49.323678Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class FloodDataset(Dataset):\n    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.transform = transform\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_path[idx]).convert('RGB')\n        mask = Image.open(self.mask_path[idx]).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        mask = mask.resize(self.image_size, Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        mask = np.clip(mask, 0, 9)\n        mask = torch.from_numpy(mask).long()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.325027Z","iopub.execute_input":"2025-12-07T02:59:49.325519Z","iopub.status.idle":"2025-12-07T02:59:49.330670Z","shell.execute_reply.started":"2025-12-07T02:59:49.325499Z","shell.execute_reply":"2025-12-07T02:59:49.330103Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"train_test_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = FloodDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\nval_dataset = FloodDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\ntest_dataset = FloodDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:59:49.331395Z","iopub.execute_input":"2025-12-07T02:59:49.331606Z","iopub.status.idle":"2025-12-07T02:59:49.346222Z","shell.execute_reply.started":"2025-12-07T02:59:49.331587Z","shell.execute_reply":"2025-12-07T02:59:49.345676Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"from transformers import (Mask2FormerForUniversalSegmentation , Mask2FormerImageProcessor, Mask2FormerConfig)\n\nmodel_id = \"facebook/mask2former-swin-large-ade-semantic\"\n\nprocessor = Mask2FormerImageProcessor.from_pretrained(\n    model_id, \n    ignore_index=255, \n    do_resize=False, \n    do_rescale=False)\n\nconfig = Mask2FormerConfig.from_pretrained(model_id)\nconfig.num_labels = NUM_CLASSES\nconfig.id2label = {i: f\"LABEL_{i}\" for i in range(NUM_CLASSES)}\nconfig.label2id = {f\"LABEL_{i}\": i for i in range(NUM_CLASSES)}\n\nmodel = Mask2FormerForUniversalSegmentation(config)\n\ncheckpoint = torch.load(DIR_MODEL+MODEL_NAME, map_location=\"cpu\")\n\nnew_state_dict = {}\nfor key, value in state_dict.items():\n    # 1. Filter head klasifikasi (yang sudah kamu lakukan)\n    if \"class_predictor\" in key:\n        continue\n    \n    # 2. Filter parameter loss yang ukurannya mismatch (TAMBAHAN INI)\n    if \"criterion.empty_weight\" in key:\n        continue\n        \n    new_state_dict[key] = value\n\nmsg = model.load_state_dict(new_state_dict, strict=False)\n\nprint(f\"Model Mask2Former (Swin Large) loaded. \\nLog: {msg}\")\n\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:08:21.938027Z","iopub.execute_input":"2025-12-07T03:08:21.938761Z","iopub.status.idle":"2025-12-07T03:08:26.520407Z","shell.execute_reply.started":"2025-12-07T03:08:21.938724Z","shell.execute_reply":"2025-12-07T03:08:26.519547Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Model Mask2Former (Swin Large) loaded. \nLog: _IncompatibleKeys(missing_keys=['class_predictor.weight', 'class_predictor.bias', 'criterion.empty_weight'], unexpected_keys=[])\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Mask2FormerForUniversalSegmentation(\n  (model): Mask2FormerModel(\n    (pixel_level_module): Mask2FormerPixelLevelModule(\n      (encoder): SwinBackbone(\n        (embeddings): SwinEmbeddings(\n          (patch_embeddings): SwinPatchEmbeddings(\n            (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n          )\n          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (encoder): SwinEncoder(\n          (layers): ModuleList(\n            (0): SwinStage(\n              (blocks): ModuleList(\n                (0): SwinLayer(\n                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=192, out_features=192, bias=True)\n                      (key): Linear(in_features=192, out_features=192, bias=True)\n                      (value): Linear(in_features=192, out_features=192, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=192, out_features=192, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): Identity()\n                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=192, out_features=768, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=768, out_features=192, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (1): SwinLayer(\n                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=192, out_features=192, bias=True)\n                      (key): Linear(in_features=192, out_features=192, bias=True)\n                      (value): Linear(in_features=192, out_features=192, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=192, out_features=192, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.013043479062616825)\n                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=192, out_features=768, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=768, out_features=192, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n              )\n              (downsample): SwinPatchMerging(\n                (reduction): Linear(in_features=768, out_features=384, bias=False)\n                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n            (1): SwinStage(\n              (blocks): ModuleList(\n                (0): SwinLayer(\n                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=384, out_features=384, bias=True)\n                      (key): Linear(in_features=384, out_features=384, bias=True)\n                      (value): Linear(in_features=384, out_features=384, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=384, out_features=384, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.02608695812523365)\n                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (1): SwinLayer(\n                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=384, out_features=384, bias=True)\n                      (key): Linear(in_features=384, out_features=384, bias=True)\n                      (value): Linear(in_features=384, out_features=384, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=384, out_features=384, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.03913043811917305)\n                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n              )\n              (downsample): SwinPatchMerging(\n                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n            (2): SwinStage(\n              (blocks): ModuleList(\n                (0): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.0521739162504673)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (1): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.06521739810705185)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (2): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.0782608762383461)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (3): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.09130435436964035)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (4): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.1043478325009346)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (5): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.11739131063222885)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (6): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.1304347962141037)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (7): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.14347827434539795)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (8): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.156521737575531)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (9): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.16956521570682526)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (10): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.1826086938381195)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (11): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.19565218687057495)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (12): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.2086956650018692)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (13): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.22173914313316345)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (14): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.2347826212644577)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (15): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.24782609939575195)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (16): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.260869562625885)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (17): SwinLayer(\n                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=768, out_features=768, bias=True)\n                      (key): Linear(in_features=768, out_features=768, bias=True)\n                      (value): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.27391305565834045)\n                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n              )\n              (downsample): SwinPatchMerging(\n                (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n                (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n            (3): SwinStage(\n              (blocks): ModuleList(\n                (0): SwinLayer(\n                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.2869565188884735)\n                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (1): SwinLayer(\n                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n                  (attention): SwinAttention(\n                    (self): SwinSelfAttention(\n                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                    (output): SwinSelfOutput(\n                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n                      (dropout): Dropout(p=0.0, inplace=False)\n                    )\n                  )\n                  (drop_path): SwinDropPath(p=0.30000001192092896)\n                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n                  (intermediate): SwinIntermediate(\n                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n                    (intermediate_act_fn): GELUActivation()\n                  )\n                  (output): SwinOutput(\n                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n              )\n            )\n          )\n        )\n        (hidden_states_norms): ModuleDict(\n          (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (decoder): Mask2FormerPixelDecoder(\n        (position_embedding): Mask2FormerSinePositionEmbedding()\n        (input_projections): ModuleList(\n          (0): Sequential(\n            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          )\n          (1): Sequential(\n            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          )\n          (2): Sequential(\n            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          )\n        )\n        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n          (layers): ModuleList(\n            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n              )\n              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (adapter_1): Sequential(\n          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (layer_1): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU()\n        )\n      )\n    )\n    (transformer_module): Mask2FormerTransformerModule(\n      (position_embedder): Mask2FormerSinePositionEmbedding()\n      (queries_embedder): Embedding(100, 256)\n      (queries_features): Embedding(100, 256)\n      (decoder): Mask2FormerMaskedAttentionDecoder(\n        (layers): ModuleList(\n          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n            (self_attn): Mask2FormerAttention(\n              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (cross_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n            )\n            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (mask_predictor): Mask2FormerMaskPredictor(\n          (mask_embedder): Mask2FormerMLPPredictionHead(\n            (0): Mask2FormerPredictionBlock(\n              (0): Linear(in_features=256, out_features=256, bias=True)\n              (1): ReLU()\n            )\n            (1): Mask2FormerPredictionBlock(\n              (0): Linear(in_features=256, out_features=256, bias=True)\n              (1): ReLU()\n            )\n            (2): Mask2FormerPredictionBlock(\n              (0): Linear(in_features=256, out_features=256, bias=True)\n              (1): Identity()\n            )\n          )\n        )\n      )\n      (level_embed): Embedding(3, 256)\n    )\n  )\n  (class_predictor): Linear(in_features=256, out_features=11, bias=True)\n  (criterion): Mask2FormerLoss(\n    (matcher): Mask2FormerHungarianMatcher()\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Train n Eval","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\nmodel.to(device)\nprint(f\"Model {model_id} siap untuk training 11 kelas RescueNet.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:08:58.185579Z","iopub.execute_input":"2025-12-07T03:08:58.186311Z","iopub.status.idle":"2025-12-07T03:08:58.197663Z","shell.execute_reply.started":"2025-12-07T03:08:58.186274Z","shell.execute_reply":"2025-12-07T03:08:58.197053Z"}},"outputs":[{"name":"stdout","text":"cuda\nModel facebook/mask2former-swin-large-ade-semantic siap untuk training 11 kelas RescueNet.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\nprint(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:08:59.539630Z","iopub.execute_input":"2025-12-07T03:08:59.539907Z","iopub.status.idle":"2025-12-07T03:08:59.544106Z","shell.execute_reply.started":"2025-12-07T03:08:59.539888Z","shell.execute_reply":"2025-12-07T03:08:59.543349Z"}},"outputs":[{"name":"stdout","text":"Train Images: 116, Train Masks: 116\nVal Images: 14, Val Masks: 14\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex\nfrom torch.optim.lr_scheduler import LambdaLR\nimport numpy as np\n\nval_iou_metric = JaccardIndex(\n    task=\"multiclass\",\n    num_classes=NUM_CLASSES,\n    ignore_index=255\n).to(device)\n\nbetas = (0.9, 0.999)\nweight_decay = 0.05\nlr=1e-5\n\n# EPOCHS = 6\n\noptimizer = AdamW(model.parameters(), \n                  # weight_decay=weight_decay, \n                  # betas=betas, \n                  lr=lr)\n\nbest_val_miou = 0.0  \nbest_epoch = -1\nglobal_iter = 0\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_miou\": []\n}\n\nprint(\" Mulai Training Mask2Former...\")\n\nfor epoch in range(EPOCHS):\n    \n    model.train()\n    \n    epoch_train_loss = 0.0\n    train_bar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{EPOCHS}\")\n\n    for images, masks in train_bar:\n\n        list_images = [img for img in images]\n        list_masks  = [m for m in masks]\n\n        inputs = processor(\n            images=list_images,\n            segmentation_maps=list_masks,\n            task_inputs=[\"semantic\"] * len(images),\n            return_tensors=\"pt\"\n        )\n\n        pixel_values = inputs[\"pixel_values\"].to(device)\n        mask_labels  = [m.to(device) for m in inputs[\"mask_labels\"]]\n        class_labels = [c.to(device) for c in inputs[\"class_labels\"]]\n\n        outputs = model(\n            pixel_values=pixel_values,\n            mask_labels=mask_labels,\n            class_labels=class_labels\n        )\n\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        \n        optimizer.step()\n        global_iter += 1\n        \n        epoch_train_loss += loss.item()\n        train_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        del inputs, pixel_values, mask_labels, class_labels, outputs, loss\n        torch.cuda.empty_cache()\n        \n\n    avg_train_loss = epoch_train_loss / len(train_loader)\n    print(f\" Train Loss: {avg_train_loss:.4f}\")\n    history[\"train_loss\"].append(avg_train_loss)\n\n    # ----------------------------\n    # VALIDATION\n    # ----------------------------\n\n    model.eval()\n    val_iou_metric.reset()\n    epoch_val_loss = 0.0\n\n    val_bar = tqdm(val_loader, desc=f\"[Val] Epoch {epoch+1}/{EPOCHS}\")\n\n    with torch.no_grad():\n        for images, masks in val_bar:\n\n            list_images = [img for img in images]\n            list_masks  = [m for m in masks]\n\n            inputs = processor(\n                images=list_images,\n                segmentation_maps=list_masks,\n                task_inputs=[\"semantic\"] * len(images),\n                return_tensors=\"pt\"\n            )\n\n            pixel_values = inputs[\"pixel_values\"].to(device)\n            mask_labels  = [m.to(device) for m in inputs[\"mask_labels\"]]\n            class_labels = [c.to(device) for c in inputs[\"class_labels\"]]\n\n            outputs = model(\n                pixel_values=pixel_values,\n                mask_labels=mask_labels,\n                class_labels=class_labels\n            )\n\n            loss = outputs.loss\n            epoch_val_loss += loss.item()\n\n            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n            preds = processor.post_process_semantic_segmentation(\n                outputs, target_sizes=target_sizes\n            )\n\n            preds_tensor  = torch.stack(preds).to(device)\n            target_tensor = masks.to(device)\n\n            val_iou_metric.update(preds_tensor, target_tensor)\n\n            del inputs, outputs, loss\n            torch.cuda.empty_cache()\n\n    avg_val_loss = epoch_val_loss / len(val_loader)\n    val_miou = val_iou_metric.compute().mean().item()\n    val_iou_metric.reset()\n    \n    print(f\" Val Loss: {avg_val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n    print(\"-\" * 50)\n\n    history[\"val_loss\"].append(avg_val_loss)\n    history[\"val_miou\"].append(val_miou)\n\n    if val_miou > best_val_miou:\n        best_val_miou = val_miou\n        best_epoch = epoch + 1\n        torch.save(model.state_dict(), \"best_model_mask2former_finetuned.pth\")\n        print(f\" New best model ! Epoch {epoch+1}, mIoU={val_miou:.4f}\")\n\nprint(f\"\\nTraining Done Om! Best model in epoch {best_epoch} with mIoU={best_val_miou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:09:00.875704Z","iopub.execute_input":"2025-12-07T03:09:00.875990Z","iopub.status.idle":"2025-12-07T03:41:22.932781Z","shell.execute_reply.started":"2025-12-07T03:09:00.875968Z","shell.execute_reply":"2025-12-07T03:41:22.932157Z"}},"outputs":[{"name":"stdout","text":" Mulai Training Mask2Former...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 1/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724c6c52c7d14c19b25922db8c7d4145"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 82.3701\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 1/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6411a7ae289a466a819421f87164f071"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 72.1589 | Val mIoU: 0.1744\n--------------------------------------------------\n New best model ! Epoch 1, mIoU=0.1744\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 2/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e23b1808eda349edadefabb324fd966a"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 65.6791\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 2/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf8501ab68134559881fa4b0117b7caa"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 66.0952 | Val mIoU: 0.2108\n--------------------------------------------------\n New best model ! Epoch 2, mIoU=0.2108\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 3/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24764274ff8f41ac804bb8949820a020"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 59.6409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 3/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08bd1684549944b7828f9eae1ff5e353"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 61.5284 | Val mIoU: 0.2162\n--------------------------------------------------\n New best model ! Epoch 3, mIoU=0.2162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 4/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6973b82f9d474efaaca267bd3b098ea2"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 55.2024\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 4/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fa6f6517854b769f1a3f887fbf950a"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 58.9421 | Val mIoU: 0.2159\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 5/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ed5c797cb948638faefafcd3e933f0"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 51.8190\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 5/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75c944918d94772a679668daadd593c"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 56.1386 | Val mIoU: 0.2406\n--------------------------------------------------\n New best model ! Epoch 5, mIoU=0.2406\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 6/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211c89d6432b4b62aeb9d03fb3c79481"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 48.6224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 6/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a227a0f34b462e868651de2ec9863b"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 53.9252 | Val mIoU: 0.2595\n--------------------------------------------------\n New best model ! Epoch 6, mIoU=0.2595\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 7/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee7921699264f89a704621af362485f"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 45.9536\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 7/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753a6d072d7a4a0d9e4aadd7015c3aca"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 51.7481 | Val mIoU: 0.2820\n--------------------------------------------------\n New best model ! Epoch 7, mIoU=0.2820\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 8/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f7acce38d74b54bbe369b2decc5eb6"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 43.8374\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 8/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d20b51191040cb923f3ce550529911"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 49.7362 | Val mIoU: 0.3362\n--------------------------------------------------\n New best model ! Epoch 8, mIoU=0.3362\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 9/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00bb882fcffe4d6cb551fad63e27b0ed"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 41.2203\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 9/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7034510774b48509b456791c9705760"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 48.5690 | Val mIoU: 0.3847\n--------------------------------------------------\n New best model ! Epoch 9, mIoU=0.3847\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 10/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ecb1b35ebf4d7ba6330338e2fe2a3d"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 39.4667\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 10/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68723c9efe434651a566181304015d15"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 47.0114 | Val mIoU: 0.4215\n--------------------------------------------------\n New best model ! Epoch 10, mIoU=0.4215\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 11/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c3c62142c244508a64891ac91e6a9c"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 37.5513\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 11/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac8d99729e747859888c637e85087a6"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 46.2175 | Val mIoU: 0.4085\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 12/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a121b28e304a3397e0805c455cb27b"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 36.2050\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 12/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c1278ab8334a69815bd4df4d77d3b7"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 46.3969 | Val mIoU: 0.4299\n--------------------------------------------------\n New best model ! Epoch 12, mIoU=0.4299\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 13/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cedd7d23a064f3fa15cb5fe11035fdc"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 34.6841\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 13/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8591000466f24407a625643c0f1f68a7"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 44.8181 | Val mIoU: 0.4351\n--------------------------------------------------\n New best model ! Epoch 13, mIoU=0.4351\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 14/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73f7370b546422d8b5a0b1d1220f44b"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 33.3573\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 14/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab4263a9715545b38adbf9b66ee1aebc"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 45.9874 | Val mIoU: 0.4104\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 15/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9a79f2748348db8394c636345a7299"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 32.1504\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 15/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"020ab77f6d9f45c0ae9f1427b8d10473"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 43.7380 | Val mIoU: 0.4773\n--------------------------------------------------\n New best model ! Epoch 15, mIoU=0.4773\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 16/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f733ba52795845878f80735946b9daf9"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 31.4690\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 16/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee7a8bcc9dec4c458b8effe496e8df57"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 42.2680 | Val mIoU: 0.4907\n--------------------------------------------------\n New best model ! Epoch 16, mIoU=0.4907\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 17/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"284156bf4c6c453a83314bfb674f80ca"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 30.3711\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 17/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b15749c21064470ebc92301b18abe4b2"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 41.5461 | Val mIoU: 0.4949\n--------------------------------------------------\n New best model ! Epoch 17, mIoU=0.4949\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 18/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9750937cb21c42749557bae52c6d37b2"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 30.2211\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 18/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187129e3a67448fcb1026239584b3777"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 40.7955 | Val mIoU: 0.5214\n--------------------------------------------------\n New best model ! Epoch 18, mIoU=0.5214\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 19/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6249ca4b1d47669d8f00a3cc31e142"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 28.7935\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 19/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8186c327f684ea08a4b7f6ecce7ef6d"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 42.0117 | Val mIoU: 0.5333\n--------------------------------------------------\n New best model ! Epoch 19, mIoU=0.5333\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 20/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3552e7d73b2f439e83e634edd664b1fb"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 28.1143\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 20/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db66aaa840fd4d669127c4544dee3ee3"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 40.7628 | Val mIoU: 0.4694\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 21/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1110853ae544e0b5237b6a212975bd"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 27.3038\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 21/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768251ce3beb407ab4b0dea89a09a3f9"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 39.9669 | Val mIoU: 0.4788\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 22/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b3d188c9c5e4a039c795113d560595a"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 26.5796\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 22/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b71cad6da1743fda1eba0bb39da4802"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 40.0534 | Val mIoU: 0.4804\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 23/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a5f357adaf45329407a1bce6a70cbb"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 26.0519\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 23/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f52409562c40f9b2201f169c50b647"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 40.2270 | Val mIoU: 0.4936\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 24/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84a4cc67eb342adae7dfe939f222b4d"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 25.7775\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 24/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fa49312dd042e490b64998bad9dbe6"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 40.0970 | Val mIoU: 0.4727\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train] Epoch 25/25:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f04b464f7be4cc68d7e3ae981942028"}},"metadata":{}},{"name":"stdout","text":" Train Loss: 25.1021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Val] Epoch 25/25:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266de9b845214fe79c2f8e75a90e5559"}},"metadata":{}},{"name":"stdout","text":" Val Loss: 39.1776 | Val mIoU: 0.4973\n--------------------------------------------------\n\nTraining Done Om! Best model in epoch 19 with mIoU=0.5333\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import json \n\nWORKDIR = \"/kaggle/working\"\noutput_path = WORKDIR + \"/history_mask2former_finetuned.json\"\n\nwith open(output_path, \"w\") as f:\n    json.dump(history, f, indent=4)\n\nprint(\"File saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:44:43.555513Z","iopub.execute_input":"2025-12-07T03:44:43.556195Z","iopub.status.idle":"2025-12-07T03:44:43.561527Z","shell.execute_reply.started":"2025-12-07T03:44:43.556174Z","shell.execute_reply":"2025-12-07T03:44:43.560803Z"}},"outputs":[{"name":"stdout","text":"File saved to: /kaggle/working/history_mask2former_finetuned.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex  \n\ndef test_model(model, test_loader, device, processor):\n    metric = JaccardIndex(\n        task=\"multiclass\", \n        num_classes=NUM_CLASSES, \n        ignore_index=255,\n        average=\"none\" \n    ).to(device)\n\n    model.to(device)\n    \n    model.eval()\n    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n            list_images = [img for img in images]\n            \n            inputs = processor(\n                images=list_images,\n                return_tensors=\"pt\",\n                do_resize=False,   \n                do_rescale=False   \n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            outputs = model(**inputs)\n            \n            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n            pred_maps = processor.post_process_semantic_segmentation(\n                outputs, target_sizes=target_sizes\n            )\n            preds_batch = torch.stack(pred_maps).to(device)\n            target_batch = masks.to(device)\n            \n            metric.update(preds_batch, target_batch)\n    \n    iou_per_class = metric.compute()\n    \n    mIoU = iou_per_class.mean().item()\n    \n    print(\"\\n=== HASIL TESTING ===\")\n    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n    print(\"-\" * 30)\n    \n    class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n    for i, iou in enumerate(iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n        \n    metric.reset()\n    return mIoU, iou_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:44:45.353939Z","iopub.execute_input":"2025-12-07T03:44:45.354230Z","iopub.status.idle":"2025-12-07T03:44:45.365082Z","shell.execute_reply.started":"2025-12-07T03:44:45.354208Z","shell.execute_reply":"2025-12-07T03:44:45.364125Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n\nmodel_id = \"facebook/mask2former-swin-large-ade-semantic\"\nWORKDIR = \"/kaggle/working\"\n\nclass_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n               \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \nprocessor = Mask2FormerImageProcessor.from_pretrained(\n    model_id,\n    ignore_index=255,\n    do_resize=False,\n    do_rescale=False\n)\n\nconfig = Mask2FormerConfig.from_pretrained(model_id)\nconfig.num_labels = NUM_CLASSES # Pastikan ini angka yang sama saat finetune!\nconfig.id2label = {i: f\"LABEL_{i}\" for i in range(NUM_CLASSES)}\nconfig.label2id = {f\"LABEL_{i}\": i for i in range(NUM_CLASSES)}\n\nmodel = Mask2FormerForUniversalSegmentation(config)\n\nckpt = torch.load(WORKDIR + \"/best_model_mask2former_finetuned.pth\", map_location=\"cpu\")\n\nif isinstance(ckpt, dict):\n    if \"model\" in ckpt:\n        ckpt_state = ckpt[\"model\"]\n    elif \"state_dict\" in ckpt:\n        ckpt_state = ckpt[\"state_dict\"]\n    else:\n        ckpt_state = ckpt\nelse:\n    ckpt_state = ckpt\n\nmissing, unexpected = model.load_state_dict(ckpt_state, strict=True)\n\nprint(\"Model loaded successfully for testing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:44:51.636658Z","iopub.execute_input":"2025-12-07T03:44:51.636948Z","iopub.status.idle":"2025-12-07T03:44:55.857158Z","shell.execute_reply.started":"2025-12-07T03:44:51.636928Z","shell.execute_reply":"2025-12-07T03:44:55.856513Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully for testing.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"test_mIoU, test_iou_per_class = test_model(model, test_loader, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:44:55.858151Z","iopub.execute_input":"2025-12-07T03:44:55.858371Z","iopub.status.idle":"2025-12-07T03:44:59.479959Z","shell.execute_reply.started":"2025-12-07T03:44:55.858355Z","shell.execute_reply":"2025-12-07T03:44:59.479205Z"}},"outputs":[{"name":"stdout","text":"Mulai Testing (menggunakan JaccardIndex)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249f15b045624b78b8c53ecc2e266f96"}},"metadata":{}},{"name":"stdout","text":"\n=== HASIL TESTING ===\nMean IoU (mIoU): 0.4478\n------------------------------\nBackground               : 0.2936\nBuilding Flooded         : 0.8513\nBuilding Non-Flooded     : 0.7857\nRoad Flooded             : 0.3896\nRoad Non-Flooded         : 0.1698\nWater                    : 0.3227\nTree                     : 0.7109\nVehicle                  : 0.1860\nPool                     : 0.0000\nGrass                    : 0.7683\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# import os\n# from matplotlib.patches import Patch\n\n# CLASS_NAMES = [\n#     \"Background\",               \n#     \"Water\",                    \n#     \"Building No Damage\",       \n#     \"Building Minor Damage\",    \n#     \"Building Major Damage\",    \n#     \"Building Total Destruction\",\n#     \"Road-Clear\",               \n#     \"Road-Blocked\",             \n#     \"Vehicle\",                  \n#     \"Tree\",                     \n#     \"Pool\"                      \n# ]\n\n# LABEL_COLORS = np.array([\n#     [0, 0, 0],         # Background \n#     [30, 230, 255],    # Water \n#     [184, 115, 117],   # Building No Damage\n#     [216, 255, 0],     # Building Minor Damage\n#     [252, 199, 0],     # Building Major Damage\n#     [255, 0, 0],       # Building Total Destruction\n#     [140, 140, 140],   # Road-Clear\n#     [151, 0, 255],     # Road-Blocked\n#     [255, 0, 246],     # Vehicle \n#     [0, 255, 0],       # Tree\n#     [244, 255, 0]      # Pool\n# ])\n# def decode_segmap(mask):\n#     r = np.zeros_like(mask).astype(np.uint8)\n#     g = np.zeros_like(mask).astype(np.uint8)\n#     b = np.zeros_like(mask).astype(np.uint8)\n    \n#     for l in range(0, len(LABEL_COLORS)):\n#         idx = mask == l\n#         r[idx] = LABEL_COLORS[l, 0]\n#         g[idx] = LABEL_COLORS[l, 1]\n#         b[idx] = LABEL_COLORS[l, 2]\n        \n#     rgb = np.stack([r, g, b], axis=2)\n#     return rgb\n\n# def find_indices_by_filename(dataset, target_ids):\n#     found_indices = []\n#     for target in target_ids:\n#         found = False\n#         for idx, path in enumerate(dataset.image_path):\n#             if str(target) in os.path.basename(path):\n#                 found_indices.append(idx)\n#                 found = True\n#                 break\n#         if not found:\n#             return \n#     return found_indices\n\n# def visualize_specific_images(model, dataset, target_ids, device, processor):\n#     model.eval()\n    \n#     indices = find_indices_by_filename(dataset, target_ids)\n\n#     num_samples = len(indices)\n#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    \n#     if num_samples == 1:\n#         axes = axes.reshape(1, -1)\n\n#     for row_idx, idx in enumerate(indices):\n#         image, mask = dataset[idx] \n        \n#         filename = os.path.basename(dataset.image_path[idx])\n        \n#         inputs = processor(\n#             images=[image], \n#             return_tensors=\"pt\",\n#             do_resize=False, \n#             do_rescale=False\n#         )\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         with torch.no_grad():\n#             outputs = model(**inputs)\n        \n#         target_sizes = [(mask.shape[0], mask.shape[1])]\n#         pred_map = processor.post_process_semantic_segmentation(\n#             outputs, target_sizes=target_sizes\n#         )[0] \n        \n#         img_np = image.permute(1, 2, 0).numpy()\n        \n#         mask_rgb = decode_segmap(mask.numpy())\n#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n        \n#         axes[row_idx, 0].imshow(img_np)\n#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n#         axes[row_idx, 0].axis(\"off\")\n        \n#         axes[row_idx, 1].imshow(mask_rgb)\n#         axes[row_idx, 1].set_title(\"Ground Truth\")\n#         axes[row_idx, 1].axis(\"off\")\n        \n#         axes[row_idx, 2].imshow(pred_rgb)\n#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n#         axes[row_idx, 2].axis(\"off\")\n\n#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n\n#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n    \n#     plt.tight_layout()\n#     plt.subplots_adjust(bottom=0.08) \n#     plt.show()\n\n# target_ids = [\"10794\", \"10801\", \"10807\"]\n\n# visualize_specific_images(model, test_dataset, target_ids, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:00:11.638637Z","iopub.status.idle":"2025-12-07T03:00:11.638942Z","shell.execute_reply.started":"2025-12-07T03:00:11.638789Z","shell.execute_reply":"2025-12-07T03:00:11.638802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# import matplotlib.pyplot as plt\n\n# test_imgs, test_masks = next(iter(test_loader))\n\n# with torch.no_grad():\n#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n    \n#     outputs = model(inputs)\n    \n#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T03:00:11.640158Z","iopub.status.idle":"2025-12-07T03:00:11.640396Z","shell.execute_reply.started":"2025-12-07T03:00:11.640264Z","shell.execute_reply":"2025-12-07T03:00:11.640273Z"}},"outputs":[],"execution_count":null}]}
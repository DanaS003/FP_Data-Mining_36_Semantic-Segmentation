{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12690938,"sourceType":"datasetVersion","datasetId":8020112}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y transformers accelerate tokenizers numpy\n\n!pip install numpy==1.26.4\n!pip install -U transformers accelerate tokenizers evaluate torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:13:16.978253Z","iopub.execute_input":"2025-12-05T04:13:16.978536Z","iopub.status.idle":"2025-12-05T04:15:01.862227Z","shell.execute_reply.started":"2025-12-05T04:13:16.978511Z","shell.execute_reply":"2025-12-05T04:15:01.861306Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy\nimport scipy\nprint(f\"Numpy version: {numpy.__version__}\")\nprint(f\"Scipy version: {scipy.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:01.863974Z","iopub.execute_input":"2025-12-05T04:15:01.864254Z","iopub.status.idle":"2025-12-05T04:15:01.885149Z","shell.execute_reply.started":"2025-12-05T04:15:01.864227Z","shell.execute_reply":"2025-12-05T04:15:01.884052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:01.886090Z","iopub.execute_input":"2025-12-05T04:15:01.887012Z","iopub.status.idle":"2025-12-05T04:15:09.343790Z","shell.execute_reply.started":"2025-12-05T04:15:01.886986Z","shell.execute_reply":"2025-12-05T04:15:09.343014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{}},{"cell_type":"code","source":"ls /kaggle/input\n\nNUM_CLASSES = 10\nBATCH_SIZE = 16\nEPOCHS = 25\n\nMODEL_NAME = \"best_model_mask2former.pth\"\nMODEL_NAME_FINETUNED = \"best_model_mask2former_finetuned.pth\"\nDIR_MODEL = \"/kaggle/input/mask2former/pytorch/default/1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:09.345508Z","iopub.execute_input":"2025-12-05T04:15:09.345969Z","iopub.status.idle":"2025-12-05T04:15:09.485206Z","shell.execute_reply.started":"2025-12-05T04:15:09.345950Z","shell.execute_reply":"2025-12-05T04:15:09.484146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport re\ndef sort_files_numerically(directory):\n    files = os.listdir(directory)\n    files_sorted = sorted(files, key=lambda x: int(re.search(r'\\d+', x).group()))\n    return [os.path.join(directory, f) for f in files_sorted]\n\nROOT_INP = \"/kaggle/input/indo-flood-segmentation-dataset\"\n\ntrain_image_paths = sort_files_numerically(ROOT_INP+'/train/train-org-img')\ntrain_mask_paths = sort_files_numerically(ROOT_INP+'/train/train-label-img')\n\nval_image_paths = sort_files_numerically(ROOT_INP+'/val/val-org-img')\nval_mask_paths = sort_files_numerically(ROOT_INP+'/val/val-label-img')\n\ntest_image_paths = sort_files_numerically(ROOT_INP+'/test/test-org-img')\ntest_mask_paths = sort_files_numerically(ROOT_INP+'/test/test-label-img')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:09.486482Z","iopub.execute_input":"2025-12-05T04:15:09.486844Z","iopub.status.idle":"2025-12-05T04:15:09.704623Z","shell.execute_reply.started":"2025-12-05T04:15:09.486807Z","shell.execute_reply":"2025-12-05T04:15:09.703794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FloodDataset(Dataset):\n    def __init__(self, image_path, mask_path, transform=None, image_size=(512, 512)):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.transform = transform\n        self.image_size = image_size\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_path[idx]).convert('RGB')\n        mask = Image.open(self.mask_path[idx]).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n\n        mask = mask.resize(self.image_size, Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        mask = np.clip(mask, 0, 9)\n        mask = torch.from_numpy(mask).long()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:09.705707Z","iopub.execute_input":"2025-12-05T04:15:09.706501Z","iopub.status.idle":"2025-12-05T04:15:09.712343Z","shell.execute_reply.started":"2025-12-05T04:15:09.706471Z","shell.execute_reply":"2025-12-05T04:15:09.711565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"train_test_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = FloodDataset(train_image_paths, train_mask_paths, transform=train_test_transform)\nval_dataset = FloodDataset(val_image_paths, val_mask_paths, transform=train_test_transform)\ntest_dataset = FloodDataset(test_image_paths, test_mask_paths, transform=train_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:09.713443Z","iopub.execute_input":"2025-12-05T04:15:09.713870Z","iopub.status.idle":"2025-12-05T04:15:09.734511Z","shell.execute_reply.started":"2025-12-05T04:15:09.713850Z","shell.execute_reply":"2025-12-05T04:15:09.733708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"from transformers import (Mask2FormerForUniversalSegmentation , Mask2FormerImageProcessor)\n\nmodel_id = \"facebook/mask2former-swin-large-ade-semantic\"\n\nprocessor = Mask2FormerImageProcessor.from_pretrained(\n    model_id, \n    ignore_index=255, \n    do_resize=False, \n    do_rescale=False)\n\nconfig = Mask2FormerConfig.from_pretrained(model_id)\nconfig.num_labels = NUM_CLASSES\nconfig.id2label = {i: f\"LABEL_{i}\" for i in range(NUM_CLASSES)}\nconfig.label2id = {f\"LABEL_{i}\": i for i in range(NUM_CLASSES)}\n\nmodel = Mask2FormerForUniversalSegmentation(config)\n\ncheckpoint = torch.load(DIR_MODEL+MODEL_NAME, map_location=\"cpu\")\n\n# model = Mask2FormerForUniversalSegmentation.from_pretrained(\n#     model_id,\n#     num_labels=NUM_CLASSES, \n#     ignore_mismatched_sizes=True\n# )\n\nif 'state_dict' in checkpoint:\n    state_dict = checkpoint['state_dict']\nelif 'model' in checkpoint:\n    state_dict = checkpoint['model']\nelse:\n    state_dict = checkpoint\n\nnew_state_dict = {}\nfor key, value in state_dict.items():\n    if \"class_predictor\" in key:\n        continue\n    new_state_dict[key] = value\n\nmsg = model.load_state_dict(new_state_dict, strict=False)\n\nprint(f\"Model Mask2Former (Swin Large) loaded. \\nLog: {msg}\")\n\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:09.735325Z","iopub.execute_input":"2025-12-05T04:15:09.736329Z","iopub.status.idle":"2025-12-05T04:15:38.740367Z","shell.execute_reply.started":"2025-12-05T04:15:09.736301Z","shell.execute_reply":"2025-12-05T04:15:38.739497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train n Eval","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\nmodel.to(device)\nprint(f\"Model {model_id} siap untuk training 11 kelas RescueNet.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:38.741416Z","iopub.execute_input":"2025-12-05T04:15:38.742021Z","iopub.status.idle":"2025-12-05T04:15:39.206841Z","shell.execute_reply.started":"2025-12-05T04:15:38.741996Z","shell.execute_reply":"2025-12-05T04:15:39.205991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train Images: {len(train_image_paths)}, Train Masks: {len(train_mask_paths)}\")\nprint(f\"Val Images: {len(val_image_paths)}, Val Masks: {len(val_mask_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:39.209228Z","iopub.execute_input":"2025-12-05T04:15:39.209537Z","iopub.status.idle":"2025-12-05T04:15:39.433574Z","shell.execute_reply.started":"2025-12-05T04:15:39.209518Z","shell.execute_reply":"2025-12-05T04:15:39.432817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex\nfrom torch.optim.lr_scheduler import LambdaLR\nimport numpy as np\n\nval_iou_metric = JaccardIndex(\n    task=\"multiclass\",\n    num_classes=NUM_CLASSES,\n    ignore_index=255\n).to(device)\n\nbetas = (0.9, 0.999)\nweight_decay = 0.05\nlr=1e-5\n\n# EPOCHS = 6\n\noptimizer = AdamW(model.parameters(), \n                  # weight_decay=weight_decay, \n                  # betas=betas, \n                  lr=lr)\n\nbest_val_miou = 0.0  \nbest_epoch = -1\nglobal_iter = 0\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_miou\": []\n}\n\nprint(\"ðŸš€ Mulai Training Mask2Former...\")\n\nfor epoch in range(EPOCHS):\n    \n    model.train()\n    \n    epoch_train_loss = 0.0\n    train_bar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{EPOCHS}\")\n\n    for images, masks in train_bar:\n\n        list_images = [img for img in images]\n        list_masks  = [m for m in masks]\n\n        inputs = processor(\n            images=list_images,\n            segmentation_maps=list_masks,\n            task_inputs=[\"semantic\"] * len(images),\n            return_tensors=\"pt\"\n        )\n\n        pixel_values = inputs[\"pixel_values\"].to(device)\n        mask_labels  = [m.to(device) for m in inputs[\"mask_labels\"]]\n        class_labels = [c.to(device) for c in inputs[\"class_labels\"]]\n\n        outputs = model(\n            pixel_values=pixel_values,\n            mask_labels=mask_labels,\n            class_labels=class_labels\n        )\n\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        \n        optimizer.step()\n        global_iter += 1\n        \n        epoch_train_loss += loss.item()\n        train_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        del inputs, pixel_values, mask_labels, class_labels, outputs, loss\n        torch.cuda.empty_cache()\n        \n\n    avg_train_loss = epoch_train_loss / len(train_loader)\n    print(f\"ðŸŽ¯ Train Loss: {avg_train_loss:.4f}\")\n    history[\"train_loss\"].append(avg_train_loss)\n\n    # ----------------------------\n    # VALIDATION\n    # ----------------------------\n\n    model.eval()\n    val_iou_metric.reset()\n    epoch_val_loss = 0.0\n\n    val_bar = tqdm(val_loader, desc=f\"[Val] Epoch {epoch+1}/{EPOCHS}\")\n\n    with torch.no_grad():\n        for images, masks in val_bar:\n\n            list_images = [img for img in images]\n            list_masks  = [m for m in masks]\n\n            inputs = processor(\n                images=list_images,\n                segmentation_maps=list_masks,\n                task_inputs=[\"semantic\"] * len(images),\n                return_tensors=\"pt\"\n            )\n\n            pixel_values = inputs[\"pixel_values\"].to(device)\n            mask_labels  = [m.to(device) for m in inputs[\"mask_labels\"]]\n            class_labels = [c.to(device) for c in inputs[\"class_labels\"]]\n\n            outputs = model(\n                pixel_values=pixel_values,\n                mask_labels=mask_labels,\n                class_labels=class_labels\n            )\n\n            loss = outputs.loss\n            epoch_val_loss += loss.item()\n\n            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n            preds = processor.post_process_semantic_segmentation(\n                outputs, target_sizes=target_sizes\n            )\n\n            preds_tensor  = torch.stack(preds).to(device)\n            target_tensor = masks.to(device)\n\n            val_iou_metric.update(preds_tensor, target_tensor)\n\n            del inputs, outputs, loss\n            torch.cuda.empty_cache()\n\n    avg_val_loss = epoch_val_loss / len(val_loader)\n    val_miou = val_iou_metric.compute().mean().item()\n    val_iou_metric.reset()\n    \n    print(f\"ðŸ“Œ Val Loss: {avg_val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n    print(\"-\" * 50)\n\n    history[\"val_loss\"].append(avg_val_loss)\n    history[\"val_miou\"].append(val_miou)\n\n    if val_miou > best_val_miou:\n        best_val_miou = val_miou\n        best_epoch = epoch + 1\n        torch.save(model.state_dict(), \"best_model_mask2former_finetuned.pth\")\n        print(f\"ðŸ’¾ New best model ! Epoch {epoch+1}, mIoU={val_miou:.4f}\")\n\nprint(f\"\\nTraining Done Om! Best model in epoch {best_epoch} with mIoU={best_val_miou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:16:05.499122Z","iopub.execute_input":"2025-12-05T04:16:05.499778Z","iopub.status.idle":"2025-12-05T07:47:23.400343Z","shell.execute_reply.started":"2025-12-05T04:16:05.499751Z","shell.execute_reply":"2025-12-05T07:47:23.398315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json \n\nWORKDIR = \"/kaggle/working\"\noutput_path = WORKDIR + \"/history_mask2former_finetuned.json\"\n\nwith open(output_path, \"w\") as f:\n    json.dump(history, f, indent=4)\n\nprint(\"File saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:48:32.178132Z","iopub.execute_input":"2025-12-05T07:48:32.179609Z","iopub.status.idle":"2025-12-05T07:48:32.187201Z","shell.execute_reply.started":"2025-12-05T07:48:32.179568Z","shell.execute_reply":"2025-12-05T07:48:32.186456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torchmetrics import JaccardIndex  \n\ndef test_model(model, test_loader, device, processor):\n    metric = JaccardIndex(\n        task=\"multiclass\", \n        num_classes=NUM_CLASSES, \n        ignore_index=255,\n        average=\"none\" \n    ).to(device)\n\n    model.to(device)\n    \n    model.eval()\n    print(\"Mulai Testing (menggunakan JaccardIndex)...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n            list_images = [img for img in images]\n            \n            inputs = processor(\n                images=list_images,\n                return_tensors=\"pt\",\n                do_resize=False,   \n                do_rescale=False   \n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            outputs = model(**inputs)\n            \n            target_sizes = [(m.shape[0], m.shape[1]) for m in masks]\n            pred_maps = processor.post_process_semantic_segmentation(\n                outputs, target_sizes=target_sizes\n            )\n            preds_batch = torch.stack(pred_maps).to(device)\n            target_batch = masks.to(device)\n            \n            metric.update(preds_batch, target_batch)\n    \n    iou_per_class = metric.compute()\n    \n    mIoU = iou_per_class.mean().item()\n    \n    print(\"\\n=== HASIL TESTING ===\")\n    print(f\"Mean IoU (mIoU): {mIoU:.4f}\")\n    print(\"-\" * 30)\n    \n    class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n                   \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n    for i, iou in enumerate(iou_per_class):\n        name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n        print(f\"{name:25s}: {iou.item():.4f}\")\n        \n    metric.reset()\n    return mIoU, iou_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:48:34.256783Z","iopub.execute_input":"2025-12-05T07:48:34.257400Z","iopub.status.idle":"2025-12-05T07:48:34.266051Z","shell.execute_reply.started":"2025-12-05T07:48:34.257377Z","shell.execute_reply":"2025-12-05T07:48:34.265238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n\nmodel_id = \"facebook/mask2former-swin-large-ade-semantic\"\nWORKDIR = \"/kaggle/working\"\n\nclass_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\",  \n               \"Road Flooded\", \"Road Non-Flooded\", \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n    \n# num_labels = len(class_names)            \n# num_classes_internal = num_labels + 1    \n\nprocessor = Mask2FormerImageProcessor.from_pretrained(\n    model_id,\n    ignore_index=255,\n    do_resize=False,\n    do_rescale=False\n)\n\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\n    model_id,\n    num_labels=NUM_CLASSES,\n    ignore_mismatched_sizes=True\n)\n\nckpt = torch.load(WORKDIR + \"/best_model_mask2former_finetuned.pth\", map_location=\"cpu\")\n\nif isinstance(ckpt, dict) and \"model\" in ckpt:\n    ckpt_state = ckpt[\"model\"]\nelse:\n    ckpt_state = ckpt\n\nmodel_state = model.state_dict()\ncompatible = {k: v for k, v in ckpt_state.items() if (k in model_state and v.shape == model_state[k].shape)}\n\nprint(f\"Total model keys: {len(model_state)}; Compatible keys from ckpt: {len(compatible)}; Skipped keys: {len(ckpt_state)-len(compatible)}\")\n\nmodel_state.update(compatible)\nmodel.load_state_dict(model_state)\n\ntest_mIoU, test_iou_per_class = test_model(model, test_loader, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:48:36.832143Z","iopub.execute_input":"2025-12-05T07:48:36.832439Z","iopub.status.idle":"2025-12-05T07:50:03.804305Z","shell.execute_reply.started":"2025-12-05T07:48:36.832416Z","shell.execute_reply":"2025-12-05T07:50:03.803346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# import os\n# from matplotlib.patches import Patch\n\n# CLASS_NAMES = [\n#     \"Background\",               \n#     \"Water\",                    \n#     \"Building No Damage\",       \n#     \"Building Minor Damage\",    \n#     \"Building Major Damage\",    \n#     \"Building Total Destruction\",\n#     \"Road-Clear\",               \n#     \"Road-Blocked\",             \n#     \"Vehicle\",                  \n#     \"Tree\",                     \n#     \"Pool\"                      \n# ]\n\n# LABEL_COLORS = np.array([\n#     [0, 0, 0],         # Background \n#     [30, 230, 255],    # Water \n#     [184, 115, 117],   # Building No Damage\n#     [216, 255, 0],     # Building Minor Damage\n#     [252, 199, 0],     # Building Major Damage\n#     [255, 0, 0],       # Building Total Destruction\n#     [140, 140, 140],   # Road-Clear\n#     [151, 0, 255],     # Road-Blocked\n#     [255, 0, 246],     # Vehicle \n#     [0, 255, 0],       # Tree\n#     [244, 255, 0]      # Pool\n# ])\n# def decode_segmap(mask):\n#     r = np.zeros_like(mask).astype(np.uint8)\n#     g = np.zeros_like(mask).astype(np.uint8)\n#     b = np.zeros_like(mask).astype(np.uint8)\n    \n#     for l in range(0, len(LABEL_COLORS)):\n#         idx = mask == l\n#         r[idx] = LABEL_COLORS[l, 0]\n#         g[idx] = LABEL_COLORS[l, 1]\n#         b[idx] = LABEL_COLORS[l, 2]\n        \n#     rgb = np.stack([r, g, b], axis=2)\n#     return rgb\n\n# def find_indices_by_filename(dataset, target_ids):\n#     found_indices = []\n#     for target in target_ids:\n#         found = False\n#         for idx, path in enumerate(dataset.image_path):\n#             if str(target) in os.path.basename(path):\n#                 found_indices.append(idx)\n#                 found = True\n#                 break\n#         if not found:\n#             return \n#     return found_indices\n\n# def visualize_specific_images(model, dataset, target_ids, device, processor):\n#     model.eval()\n    \n#     indices = find_indices_by_filename(dataset, target_ids)\n\n#     num_samples = len(indices)\n#     fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    \n#     if num_samples == 1:\n#         axes = axes.reshape(1, -1)\n\n#     for row_idx, idx in enumerate(indices):\n#         image, mask = dataset[idx] \n        \n#         filename = os.path.basename(dataset.image_path[idx])\n        \n#         inputs = processor(\n#             images=[image], \n#             return_tensors=\"pt\",\n#             do_resize=False, \n#             do_rescale=False\n#         )\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         with torch.no_grad():\n#             outputs = model(**inputs)\n        \n#         target_sizes = [(mask.shape[0], mask.shape[1])]\n#         pred_map = processor.post_process_semantic_segmentation(\n#             outputs, target_sizes=target_sizes\n#         )[0] \n        \n#         img_np = image.permute(1, 2, 0).numpy()\n        \n#         mask_rgb = decode_segmap(mask.numpy())\n#         pred_rgb = decode_segmap(pred_map.cpu().numpy())\n        \n#         axes[row_idx, 0].imshow(img_np)\n#         axes[row_idx, 0].set_title(f\"ID: {filename}\\nOriginal Image\")\n#         axes[row_idx, 0].axis(\"off\")\n        \n#         axes[row_idx, 1].imshow(mask_rgb)\n#         axes[row_idx, 1].set_title(\"Ground Truth\")\n#         axes[row_idx, 1].axis(\"off\")\n        \n#         axes[row_idx, 2].imshow(pred_rgb)\n#         axes[row_idx, 2].set_title(\"Mask2Former Prediction\")\n#         axes[row_idx, 2].axis(\"off\")\n\n#     handles = [Patch(color=LABEL_COLORS[i]/255.0, label=CLASS_NAMES[i]) for i in range(len(CLASS_NAMES))]\n#     fig.legend(handles=handles, loc='lower center', ncol=6, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n\n#     plt.savefig('visualisasi_prediksi_rescuenet.png', bbox_inches='tight', dpi=300)\n    \n#     plt.tight_layout()\n#     plt.subplots_adjust(bottom=0.08) \n#     plt.show()\n\n# target_ids = [\"10794\", \"10801\", \"10807\"]\n\n# visualize_specific_images(model, test_dataset, target_ids, device, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:39.456676Z","iopub.status.idle":"2025-12-05T04:15:39.456977Z","shell.execute_reply.started":"2025-12-05T04:15:39.456850Z","shell.execute_reply":"2025-12-05T04:15:39.456865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# import matplotlib.pyplot as plt\n\n# test_imgs, test_masks = next(iter(test_loader))\n\n# with torch.no_grad():\n#     inputs = [{\"image\": test_imgs[0].to(cfg.MODEL.DEVICE), \"height\": 512, \"width\": 512}]\n    \n#     outputs = model(inputs)\n    \n#     pred_mask = outputs[0][\"sem_seg\"].argmax(dim=0).cpu().numpy()\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1); plt.title(\"Prediction\"); plt.imshow(pred_mask)\n# plt.subplot(1, 2, 2); plt.title(\"Ground Truth\"); plt.imshow(test_masks[0])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T04:15:39.458179Z","iopub.status.idle":"2025-12-05T04:15:39.458521Z","shell.execute_reply.started":"2025-12-05T04:15:39.458348Z","shell.execute_reply":"2025-12-05T04:15:39.458364Z"}},"outputs":[],"execution_count":null}]}